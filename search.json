[
  {
    "objectID": "posts/nca/index.html",
    "href": "posts/nca/index.html",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "What we are going to do is to try to create a diffrentiable self-organising system Neural Cellular Automata with just a few hundred parameters.We set up a system of ‘cells’, often represented by pixels in a line or a grid. Each cell can ‘see’ their immediate neighbors, and can change it’s output based on this information. We ll use Cellular Automata models to identifying cell-level rules which give rise to complex, regenerative behavior of the collective.\nCellular Automatas consist of a grid of cells being iteratively updated, with the same set of rules being applied to each cell at every step.The new state of a cell depends only on the states of the few cells in its immediate neighborhood.\nSo to put it in simple words, we start out with a random grid with pure noise pixels. Each cell in the grid only knows about the states of its immediate neighbourhood cells.With this structure in place, we will produce a predefined multicellular pattern on a 2D grid all by using differentiable update rules without any global update clock.\n\nWe ll be writing our training loops,callbacks and hooks using MiniAi. MiniAI is a small and flexible library which goes under the hood, and gives us the flexibility to customize every part of model training - from the model initialization to writing hooks to look inside our model. Let’s go ahead and install MiniAi.\n\n!pip install -Uqq git+https://github.com/fastai/course22p2\n\n\npip install git+https://github.com/huggingface/transformers\n\n\n\n\nimport pickle,gzip,math,os,time,shutil,torch,random,timm,torchvision,io,PIL, einops\nimport fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom IPython.display import display, clear_output, HTML\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\nfrom fastcore.foundation import L, store_attr\nfrom PIL import Image\nimport base64\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.init import *\nfrom miniai.sgd import *\nfrom miniai.resnet import *\n\n\n\n\ndef download_image(url):\n    imgb = fc.urlread(url, decode=False) \n    return torchvision.io.decode_image(tensor(list(imgb), dtype=torch.uint8)).float()/255.\nurl = \"https://images.pexels.com/photos/34225/spider-web-with-water-beads-network-dewdrop.jpg?w=256\"\n# url = \"https://as2.ftcdn.net/v2/jpg/04/67/20/91/1000_F_467209130_vMox1GNkLxsrL4S9v3tWGoMOeNoGSJT2.jpg\"\n# url = \"https://www.robots.ox.ac.uk/~vgg/data/dtd/thumbs/dotted/dotted_0201.jpg\"\n# url = 'https://www.robots.ox.ac.uk/~vgg/data/dtd/thumbs/bubbly/bubbly_0101.jpg'\n# url = \"https://www.freevector.com/uploads/vector/preview/17677/FreeVector-Leaf-Pattern.jpg\"\n\nstyle_im = download_image(url).to(def_device)\nshow_image(style_im);\n\n\n\n\n\n\n\n\n\n\n\nWe will use Vgg16 to extract features from the style image. The resultant feature maps contain low level feature representations of the style image encoded sptially. But we don’t want that. We need encoded information of patterns, for that we need to figure out what is the co-relation between these low-level features. Think about it like this, just having individual feature maps would give you the representation of each feature (a curve, dots, edges etc), but the degree of corelation between these features encoded in a matrix would be able to identify the patterns that appear in the image. We will use Gram Matrices to compute these - for a feature map with f features in an h x w grid, we’ll flatten out the spatial component and then for every feature we’ll take the dot product of that row with itself, giving an f x f matrix as the result.\nOur loss function will be the L2 Loss between the Gram matrix of Style image, and the iteratively updated Gram matrix of the input random noise grid as keep going through the forward and backward passes of our NCA model.\nThe picture below eloquently captures what a Gram Matrix is\n\n\n\nvgg16 = timm.create_model('vgg16', pretrained=True).to(def_device).features\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\ndef calc_features(imgs, target_layers=[18, 25]): \n    x = normalize(imgs)\n    feats = []\n    for i, layer in enumerate(vgg16[:max(target_layers)+1]):\n        x = layer(x)\n        if i in target_layers:\n            feats.append(x.clone())\n    return feats\n\ndef calc_grams(img, target_layers=[1, 6, 11, 18, 25]):\n    return L(torch.einsum('bchw, bdhw -&gt; cd', x, x) / (x.shape[-2]*x.shape[-1])\n            for x in calc_features(img, target_layers))\n\nclass StyleLossToTarget():\n    def __init__(self, target_im, target_layers=[1, 6, 11, 18, 25]):\n        fc.store_attr()\n        with torch.no_grad(): self.target_grams = calc_grams(target_im[None], target_layers)\n    def __call__(self, input_im): \n        return sum((f1-f2).pow(2).mean() for f1, f2 in \n               zip(calc_grams(input_im, self.target_layers), self.target_grams))\n\nstyle_loss = StyleLossToTarget(style_im)\nstyle_loss(torch.rand(1, 3, 256, 256).to(def_device))\n\n\n\n\ntensor(1179.7140, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)\n\n\n\nvgg16\n\nSequential(\n  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU(inplace=True)\n  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (3): ReLU(inplace=True)\n  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (6): ReLU(inplace=True)\n  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (8): ReLU(inplace=True)\n  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (11): ReLU(inplace=True)\n  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (13): ReLU(inplace=True)\n  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (15): ReLU(inplace=True)\n  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (18): ReLU(inplace=True)\n  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (20): ReLU(inplace=True)\n  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (22): ReLU(inplace=True)\n  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (25): ReLU(inplace=True)\n  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (27): ReLU(inplace=True)\n  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (29): ReLU(inplace=True)\n  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\n\n\nLet’s set up our NCA Model now\n\nnum_channels = 4\nhidden_n = 8\n\nWe will have intial grids with the image size which we want and values of each cell will be 0. We make one grid with grid size of 128 and channels as defined above\n\ndef make_grids(n, sz=128): return torch.zeros(n, num_channels, sz, sz).to(def_device)\nx = make_grids(1)\nx.shape\n\ntorch.Size([1, 4, 128, 128])\n\n\nNow we will have our pre defined filters which we will aplly across our grids. These are four 3x3 filters which we will apply acorss each of the 4 channels individually\n\n# Hard-coded filters\nfilters = torch.stack([\n    tensor([[0.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,0.0]]),\n    tensor([[-1.0,0.0,1.0],[-2.0,0.0,2.0],[-1.0,0.0,1.0]]),\n    tensor([[-1.0,0.0,1.0],[-2.0,0.0,2.0],[-1.0,0.0,1.0]]).T,\n    tensor([[1.0,2.0,1.0],[2.0,-12,2.0],[1.0,2.0,1.0]])\n]).to(def_device)\nfilters.shape\n\ntorch.Size([4, 3, 3])\n\n\n\ndef perchannel_conv(x, filters):\n    '''filters: [filter_n, h, w]'''\n    b, ch, h, w = x.shape\n    y = x.reshape(b*ch, 1, h, w)\n    y = F.pad(y, [1, 1, 1, 1], 'circular') # &lt;&lt; Note pad mode\n    y = F.conv2d(y, filters[:,None])\n    return y.reshape(b, -1, h, w)\n\nWe apply these filters across every four channels of our input zero grids, to have the final 16 channels\n\nmodel_inputs = perchannel_conv(x, filters)\nmodel_inputs.shape\n\ntorch.Size([1, 16, 128, 128])\n\n\n\nfilters.shape[0]\n\n4\n\n\n\n\n\n\nWe can now go ahead and define our NCA model class\nFew things to note in the NCA class we have written - We zero out the weights of the second layer while initializing - By initializing the weights to zero, you provide a starting point that might encourage the model to learn meaningful features from the data. - Random update: only update ~50% of the cells. This is just like applying Dropout - This Random update is inspired by biology where in a biological system not all updates are done with respect to any global clock at the same instance while it grows or updates. Similarily we add randomess to all the updates in our model\n\nclass SimpleCA(nn.Module):\n    def __init__(self, zero_w2=True):\n        super().__init__()\n        self.w1 = nn.Conv2d(num_channels*4, hidden_n, 1)\n        self.relu = nn.ReLU()\n        self.w2 = nn.Conv2d(hidden_n, num_channels, 1, bias=False)\n        if zero_w2: self.w2.weight.data.zero_()\n\n\n    def forward(self, x, update_rate=0.5):\n        y = perchannel_conv(x, filters) # Apply the filters\n        y = self.w2(self.relu(self.w1(y))) # pass the result through our simple neural network\n        b, c, h, w = y.shape\n        update_mask = (torch.rand(b, 1, h, w).to(x.device)+update_rate).floor() # Random update\n        return x+y*update_mask\n\n    def to_rgb(self, x):\n        return x[...,:3,:,:]+0.5\n\n\n\n\nWe will be using MiniAi framework and its customised methods(classes,callbacks etc) to write our training loop\n\nclass LengthDataset():\n    def __init__(self, length=1): self.length=length\n    def __len__(self): return self.length\n    def __getitem__(self, idx): return 0,0\n\ndef get_dummy_dls(length=100):\n    return DataLoaders(DataLoader(LengthDataset(length), batch_size=1),\n                       DataLoader(LengthDataset(1), batch_size=1))\n\nWe following is a callback that plots graphs and training metrics using MiniAi\n\nclass NCAProgressCB(ProgressCB):\n    def after_batch(self, learn):\n        learn.dl.comment = f'{learn.loss:.3f}'\n        if not (hasattr(learn, 'metrics') and learn.training): return \n        self.losses.append(learn.loss.item())\n        mbar = self.mbar\n        if not hasattr(mbar, 'graph_fig'):\n            mbar.graph_fig, mbar.graph_axs = plt.subplots(1, 2, figsize=(12, 3.5))\n            mbar.graph_out = display(mbar.graph_fig, display_id=True)\n\n        # Update preview image every 64 iters\n        if (len(self.losses))%64 != 10: return \n        \n        # Plot losses:\n        mbar.graph_axs[0].clear()\n        mbar.graph_axs[0].plot(self.losses, '.', alpha=0.3)\n        mbar.graph_axs[0].set_yscale('log')\n        mbar.graph_axs[0].set_ylim(tensor(self.losses).min(), self.losses[0])\n        \n        # Show preview images:\n        rgb = learn.model.to_rgb(learn.preds.detach()).clip(0, 1)\n        show_image(torchvision.utils.make_grid(rgb), ax=mbar.graph_axs[1])\n        \n        # Update graph\n        mbar.graph_out.update(mbar.graph_fig)\n\nAlong with Style Loss which we defined above, we also use overflow loss which penalises predictions that overflow beyond a range (-1 to 1 in our case)\n\nclass NCACB(TrainCB):\n    order = DeviceCB.order+1\n    def __init__(self, ca, style_img_tensor, style_loss_scale=0.1, size=256, \n                 step_n_min=32, step_n_max=96, batch_size=4):\n        fc.store_attr()\n        with torch.no_grad(): self.pool = make_grids(256, sz=size) # Set up a 'pool' of grids\n    \n    def predict(self, learn): \n        \n        # Pick some random samples from the pool\n        #we select random 4 numbers to be the current batch and use those grid from the pool\n        batch_idx = torch.randint(0, len(self.pool), (self.batch_size,))\n        x = self.pool[batch_idx]\n        \n        # occasionally zero out some samples\n        #if a random number is less than 1 from selecting 1 to 7, it zeros out \n        if torch.randint(8, (1,)) &lt; 1: \n            x[:1] =  make_grids(1, sz=self.size)\n        \n        # Apply the model a number of times\n        for _ in range(torch.randint(self.step_n_min, self.step_n_max, (1,))):\n            x = learn.model(x)\n        \n        # Update pool\n        with torch.no_grad(): self.pool[batch_idx] = x\n        \n        # and store preds\n        learn.preds = x\n        \n    def get_loss(self, learn): \n        style_loss = learn.loss_func(learn.model.to_rgb(self.learn.preds))\n        overflow_loss = (learn.preds-learn.preds.clamp(-1.0, 1.0)).abs().sum()\n        learn.loss = overflow_loss + style_loss*self.style_loss_scale\n        \n    def backward(self, learn):\n        learn.loss.backward()\n        # Gradient normalization:\n        for p in learn.model.parameters():\n            p.grad /= (p.grad.norm()+1e-8) \n        \n    def before_fit(self, learn): self.learn=learn \n\n\nmodel = SimpleCA().to(def_device)\ncbs = [NCACB(model, style_im), NCAProgressCB(), MetricsCB()]\nstyle_loss = StyleLossToTarget(style_im)\n\nlearn = Learner(model, get_dummy_dls(1200), style_loss, lr=1e-3, cbs=cbs, opt_func=torch.optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n164.076\n0\ntrain\n\n\n26.250\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBelow is the final batch of images in training\n\n# Check out the final batch:\nrgb = model.to_rgb(learn.preds.detach())\nrgb = torchvision.utils.make_grid(rgb)\nshow_image(rgb.clip(0, 1));\n\n\n\n\n\n\n\n\nLet’s put our trained NCA model to use. We start of with a random noise grid as usual, and apply the model to this grid for n number of times(900 below). With a few hundred steps we see the patterns forming from our style image. If we look closely we can see how eloquently these patterns are forming, with even spacing and ending properly around edges. All this with a model with just a few hundred paramerers. Having the ability to capture regenerative, complex patterns with just few hundred parameters is the real beauty of these NCA self organising models.\n\n# Apply a numbe of times to a random initial starting grid:\nimages = []\nx = torch.randn(1, num_channels, 128, 128).to(def_device) * 0.1\nwith torch.no_grad():\n    for i in range(1200):\n        x = model(x)\n        if i%100==0: images.append(model.to_rgb(x)[0].clip(0, 1))\nshow_images(images)\n\n\n\n\n\n\n\n\nSo just with 168 weight parameters, we have a model which can create patterns of the style image from random noise\n\nsum(p.numel() for p in model.parameters()) # !!\n\n168\n\n\n\n\n\nLet’s put our images we get from our model at different timesteps, into a video to see the progress of how the patterns develop over time as the grid goes from noise to changing some pixels to finally developing mature patterns present in the Style Image.\n\ndef progress_video():\n  # Turn the images in steps/ into a video with ffmpeg\n  !ffmpeg -y -v 0 -framerate 24 -i steps/%05d.jpeg video.mp4\n\n  # Display it inline\n  mp4 = open('video.mp4','rb').read()\n  data_url = \"data:video/mp4;base64,\" + base64.b64encode(mp4).decode()\n  return HTML(\"\"\"\n  &lt;video width=256 controls&gt;\n        &lt;source src=\"%s\" type=\"video/mp4\"&gt;\n  &lt;/video&gt;\n  \"\"\" % data_url)\n\n\n!mkdir -p steps\n!rm steps/*\nx = torch.randn(1, num_channels, 128, 128).to(def_device) * 0.1\nfor i in range(900):\n  with torch.no_grad():\n    x = model(x)\n    img = model.to_rgb(x).detach().cpu().clip(0, 1).squeeze().permute(1, 2, 0)\n    img = Image.fromarray(np.array(img*255).astype(np.uint8))\n    img.save(f'steps/{i:05}.jpeg')\nprogress_video()\n\n\n  \n        \n  \n  \n\n\n\n\n\nIn the post today, we tried out the vanilla NCA model. We saw the power of self organising systems.We made pretty pictures using differentiable self-organizing systems. There is a lot of scope to make this model detect and regenerate complex persisting patterns. We only saw a small glimpse of self organising systems. We can add more kernels, make the model deeper, try out different loss function among other improvements to see how complex patterns these models can capture, and regenerate. We can erase out part of the pattern from the image and still the model will be able to regenerate the pattern with just the knowledge of the neighbouring cells. We will try this and much more in the upcoming posts."
  },
  {
    "objectID": "posts/nca/index.html#importing-libraries",
    "href": "posts/nca/index.html#importing-libraries",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "import pickle,gzip,math,os,time,shutil,torch,random,timm,torchvision,io,PIL, einops\nimport fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom IPython.display import display, clear_output, HTML\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\nfrom fastcore.foundation import L, store_attr\nfrom PIL import Image\nimport base64\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.init import *\nfrom miniai.sgd import *\nfrom miniai.resnet import *\n\n\n\n\ndef download_image(url):\n    imgb = fc.urlread(url, decode=False) \n    return torchvision.io.decode_image(tensor(list(imgb), dtype=torch.uint8)).float()/255.\nurl = \"https://images.pexels.com/photos/34225/spider-web-with-water-beads-network-dewdrop.jpg?w=256\"\n# url = \"https://as2.ftcdn.net/v2/jpg/04/67/20/91/1000_F_467209130_vMox1GNkLxsrL4S9v3tWGoMOeNoGSJT2.jpg\"\n# url = \"https://www.robots.ox.ac.uk/~vgg/data/dtd/thumbs/dotted/dotted_0201.jpg\"\n# url = 'https://www.robots.ox.ac.uk/~vgg/data/dtd/thumbs/bubbly/bubbly_0101.jpg'\n# url = \"https://www.freevector.com/uploads/vector/preview/17677/FreeVector-Leaf-Pattern.jpg\"\n\nstyle_im = download_image(url).to(def_device)\nshow_image(style_im);\n\n\n\n\n\n\n\n\n\n\n\nWe will use Vgg16 to extract features from the style image. The resultant feature maps contain low level feature representations of the style image encoded sptially. But we don’t want that. We need encoded information of patterns, for that we need to figure out what is the co-relation between these low-level features. Think about it like this, just having individual feature maps would give you the representation of each feature (a curve, dots, edges etc), but the degree of corelation between these features encoded in a matrix would be able to identify the patterns that appear in the image. We will use Gram Matrices to compute these - for a feature map with f features in an h x w grid, we’ll flatten out the spatial component and then for every feature we’ll take the dot product of that row with itself, giving an f x f matrix as the result.\nOur loss function will be the L2 Loss between the Gram matrix of Style image, and the iteratively updated Gram matrix of the input random noise grid as keep going through the forward and backward passes of our NCA model.\nThe picture below eloquently captures what a Gram Matrix is\n\n\n\nvgg16 = timm.create_model('vgg16', pretrained=True).to(def_device).features\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\ndef calc_features(imgs, target_layers=[18, 25]): \n    x = normalize(imgs)\n    feats = []\n    for i, layer in enumerate(vgg16[:max(target_layers)+1]):\n        x = layer(x)\n        if i in target_layers:\n            feats.append(x.clone())\n    return feats\n\ndef calc_grams(img, target_layers=[1, 6, 11, 18, 25]):\n    return L(torch.einsum('bchw, bdhw -&gt; cd', x, x) / (x.shape[-2]*x.shape[-1])\n            for x in calc_features(img, target_layers))\n\nclass StyleLossToTarget():\n    def __init__(self, target_im, target_layers=[1, 6, 11, 18, 25]):\n        fc.store_attr()\n        with torch.no_grad(): self.target_grams = calc_grams(target_im[None], target_layers)\n    def __call__(self, input_im): \n        return sum((f1-f2).pow(2).mean() for f1, f2 in \n               zip(calc_grams(input_im, self.target_layers), self.target_grams))\n\nstyle_loss = StyleLossToTarget(style_im)\nstyle_loss(torch.rand(1, 3, 256, 256).to(def_device))\n\n\n\n\ntensor(1179.7140, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)\n\n\n\nvgg16\n\nSequential(\n  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU(inplace=True)\n  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (3): ReLU(inplace=True)\n  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (6): ReLU(inplace=True)\n  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (8): ReLU(inplace=True)\n  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (11): ReLU(inplace=True)\n  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (13): ReLU(inplace=True)\n  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (15): ReLU(inplace=True)\n  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (18): ReLU(inplace=True)\n  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (20): ReLU(inplace=True)\n  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (22): ReLU(inplace=True)\n  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (25): ReLU(inplace=True)\n  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (27): ReLU(inplace=True)\n  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (29): ReLU(inplace=True)\n  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\n\n\nLet’s set up our NCA Model now\n\nnum_channels = 4\nhidden_n = 8\n\nWe will have intial grids with the image size which we want and values of each cell will be 0. We make one grid with grid size of 128 and channels as defined above\n\ndef make_grids(n, sz=128): return torch.zeros(n, num_channels, sz, sz).to(def_device)\nx = make_grids(1)\nx.shape\n\ntorch.Size([1, 4, 128, 128])\n\n\nNow we will have our pre defined filters which we will aplly across our grids. These are four 3x3 filters which we will apply acorss each of the 4 channels individually\n\n# Hard-coded filters\nfilters = torch.stack([\n    tensor([[0.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,0.0]]),\n    tensor([[-1.0,0.0,1.0],[-2.0,0.0,2.0],[-1.0,0.0,1.0]]),\n    tensor([[-1.0,0.0,1.0],[-2.0,0.0,2.0],[-1.0,0.0,1.0]]).T,\n    tensor([[1.0,2.0,1.0],[2.0,-12,2.0],[1.0,2.0,1.0]])\n]).to(def_device)\nfilters.shape\n\ntorch.Size([4, 3, 3])\n\n\n\ndef perchannel_conv(x, filters):\n    '''filters: [filter_n, h, w]'''\n    b, ch, h, w = x.shape\n    y = x.reshape(b*ch, 1, h, w)\n    y = F.pad(y, [1, 1, 1, 1], 'circular') # &lt;&lt; Note pad mode\n    y = F.conv2d(y, filters[:,None])\n    return y.reshape(b, -1, h, w)\n\nWe apply these filters across every four channels of our input zero grids, to have the final 16 channels\n\nmodel_inputs = perchannel_conv(x, filters)\nmodel_inputs.shape\n\ntorch.Size([1, 16, 128, 128])\n\n\n\nfilters.shape[0]\n\n4"
  },
  {
    "objectID": "posts/nca/index.html#nca-model",
    "href": "posts/nca/index.html#nca-model",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "We can now go ahead and define our NCA model class\nFew things to note in the NCA class we have written - We zero out the weights of the second layer while initializing - By initializing the weights to zero, you provide a starting point that might encourage the model to learn meaningful features from the data. - Random update: only update ~50% of the cells. This is just like applying Dropout - This Random update is inspired by biology where in a biological system not all updates are done with respect to any global clock at the same instance while it grows or updates. Similarily we add randomess to all the updates in our model\n\nclass SimpleCA(nn.Module):\n    def __init__(self, zero_w2=True):\n        super().__init__()\n        self.w1 = nn.Conv2d(num_channels*4, hidden_n, 1)\n        self.relu = nn.ReLU()\n        self.w2 = nn.Conv2d(hidden_n, num_channels, 1, bias=False)\n        if zero_w2: self.w2.weight.data.zero_()\n\n\n    def forward(self, x, update_rate=0.5):\n        y = perchannel_conv(x, filters) # Apply the filters\n        y = self.w2(self.relu(self.w1(y))) # pass the result through our simple neural network\n        b, c, h, w = y.shape\n        update_mask = (torch.rand(b, 1, h, w).to(x.device)+update_rate).floor() # Random update\n        return x+y*update_mask\n\n    def to_rgb(self, x):\n        return x[...,:3,:,:]+0.5"
  },
  {
    "objectID": "posts/nca/index.html#lets-get-training",
    "href": "posts/nca/index.html#lets-get-training",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "We will be using MiniAi framework and its customised methods(classes,callbacks etc) to write our training loop\n\nclass LengthDataset():\n    def __init__(self, length=1): self.length=length\n    def __len__(self): return self.length\n    def __getitem__(self, idx): return 0,0\n\ndef get_dummy_dls(length=100):\n    return DataLoaders(DataLoader(LengthDataset(length), batch_size=1),\n                       DataLoader(LengthDataset(1), batch_size=1))\n\nWe following is a callback that plots graphs and training metrics using MiniAi\n\nclass NCAProgressCB(ProgressCB):\n    def after_batch(self, learn):\n        learn.dl.comment = f'{learn.loss:.3f}'\n        if not (hasattr(learn, 'metrics') and learn.training): return \n        self.losses.append(learn.loss.item())\n        mbar = self.mbar\n        if not hasattr(mbar, 'graph_fig'):\n            mbar.graph_fig, mbar.graph_axs = plt.subplots(1, 2, figsize=(12, 3.5))\n            mbar.graph_out = display(mbar.graph_fig, display_id=True)\n\n        # Update preview image every 64 iters\n        if (len(self.losses))%64 != 10: return \n        \n        # Plot losses:\n        mbar.graph_axs[0].clear()\n        mbar.graph_axs[0].plot(self.losses, '.', alpha=0.3)\n        mbar.graph_axs[0].set_yscale('log')\n        mbar.graph_axs[0].set_ylim(tensor(self.losses).min(), self.losses[0])\n        \n        # Show preview images:\n        rgb = learn.model.to_rgb(learn.preds.detach()).clip(0, 1)\n        show_image(torchvision.utils.make_grid(rgb), ax=mbar.graph_axs[1])\n        \n        # Update graph\n        mbar.graph_out.update(mbar.graph_fig)\n\nAlong with Style Loss which we defined above, we also use overflow loss which penalises predictions that overflow beyond a range (-1 to 1 in our case)\n\nclass NCACB(TrainCB):\n    order = DeviceCB.order+1\n    def __init__(self, ca, style_img_tensor, style_loss_scale=0.1, size=256, \n                 step_n_min=32, step_n_max=96, batch_size=4):\n        fc.store_attr()\n        with torch.no_grad(): self.pool = make_grids(256, sz=size) # Set up a 'pool' of grids\n    \n    def predict(self, learn): \n        \n        # Pick some random samples from the pool\n        #we select random 4 numbers to be the current batch and use those grid from the pool\n        batch_idx = torch.randint(0, len(self.pool), (self.batch_size,))\n        x = self.pool[batch_idx]\n        \n        # occasionally zero out some samples\n        #if a random number is less than 1 from selecting 1 to 7, it zeros out \n        if torch.randint(8, (1,)) &lt; 1: \n            x[:1] =  make_grids(1, sz=self.size)\n        \n        # Apply the model a number of times\n        for _ in range(torch.randint(self.step_n_min, self.step_n_max, (1,))):\n            x = learn.model(x)\n        \n        # Update pool\n        with torch.no_grad(): self.pool[batch_idx] = x\n        \n        # and store preds\n        learn.preds = x\n        \n    def get_loss(self, learn): \n        style_loss = learn.loss_func(learn.model.to_rgb(self.learn.preds))\n        overflow_loss = (learn.preds-learn.preds.clamp(-1.0, 1.0)).abs().sum()\n        learn.loss = overflow_loss + style_loss*self.style_loss_scale\n        \n    def backward(self, learn):\n        learn.loss.backward()\n        # Gradient normalization:\n        for p in learn.model.parameters():\n            p.grad /= (p.grad.norm()+1e-8) \n        \n    def before_fit(self, learn): self.learn=learn \n\n\nmodel = SimpleCA().to(def_device)\ncbs = [NCACB(model, style_im), NCAProgressCB(), MetricsCB()]\nstyle_loss = StyleLossToTarget(style_im)\n\nlearn = Learner(model, get_dummy_dls(1200), style_loss, lr=1e-3, cbs=cbs, opt_func=torch.optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n164.076\n0\ntrain\n\n\n26.250\n0\neval"
  },
  {
    "objectID": "posts/nca/index.html#results",
    "href": "posts/nca/index.html#results",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "Below is the final batch of images in training\n\n# Check out the final batch:\nrgb = model.to_rgb(learn.preds.detach())\nrgb = torchvision.utils.make_grid(rgb)\nshow_image(rgb.clip(0, 1));\n\n\n\n\n\n\n\n\nLet’s put our trained NCA model to use. We start of with a random noise grid as usual, and apply the model to this grid for n number of times(900 below). With a few hundred steps we see the patterns forming from our style image. If we look closely we can see how eloquently these patterns are forming, with even spacing and ending properly around edges. All this with a model with just a few hundred paramerers. Having the ability to capture regenerative, complex patterns with just few hundred parameters is the real beauty of these NCA self organising models.\n\n# Apply a numbe of times to a random initial starting grid:\nimages = []\nx = torch.randn(1, num_channels, 128, 128).to(def_device) * 0.1\nwith torch.no_grad():\n    for i in range(1200):\n        x = model(x)\n        if i%100==0: images.append(model.to_rgb(x)[0].clip(0, 1))\nshow_images(images)\n\n\n\n\n\n\n\n\nSo just with 168 weight parameters, we have a model which can create patterns of the style image from random noise\n\nsum(p.numel() for p in model.parameters()) # !!\n\n168"
  },
  {
    "objectID": "posts/nca/index.html#video",
    "href": "posts/nca/index.html#video",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "Let’s put our images we get from our model at different timesteps, into a video to see the progress of how the patterns develop over time as the grid goes from noise to changing some pixels to finally developing mature patterns present in the Style Image.\n\ndef progress_video():\n  # Turn the images in steps/ into a video with ffmpeg\n  !ffmpeg -y -v 0 -framerate 24 -i steps/%05d.jpeg video.mp4\n\n  # Display it inline\n  mp4 = open('video.mp4','rb').read()\n  data_url = \"data:video/mp4;base64,\" + base64.b64encode(mp4).decode()\n  return HTML(\"\"\"\n  &lt;video width=256 controls&gt;\n        &lt;source src=\"%s\" type=\"video/mp4\"&gt;\n  &lt;/video&gt;\n  \"\"\" % data_url)\n\n\n!mkdir -p steps\n!rm steps/*\nx = torch.randn(1, num_channels, 128, 128).to(def_device) * 0.1\nfor i in range(900):\n  with torch.no_grad():\n    x = model(x)\n    img = model.to_rgb(x).detach().cpu().clip(0, 1).squeeze().permute(1, 2, 0)\n    img = Image.fromarray(np.array(img*255).astype(np.uint8))\n    img.save(f'steps/{i:05}.jpeg')\nprogress_video()"
  },
  {
    "objectID": "posts/nca/index.html#conclusion",
    "href": "posts/nca/index.html#conclusion",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "In the post today, we tried out the vanilla NCA model. We saw the power of self organising systems.We made pretty pictures using differentiable self-organizing systems. There is a lot of scope to make this model detect and regenerate complex persisting patterns. We only saw a small glimpse of self organising systems. We can add more kernels, make the model deeper, try out different loss function among other improvements to see how complex patterns these models can capture, and regenerate. We can erase out part of the pattern from the image and still the model will be able to regenerate the pattern with just the knowledge of the neighbouring cells. We will try this and much more in the upcoming posts."
  },
  {
    "objectID": "posts/attention_in_vision/index.html",
    "href": "posts/attention_in_vision/index.html",
    "title": "Unpacking Attention & Transformers in Vision - From Theory to Implementation",
    "section": "",
    "text": "Attention is the driving force behind the latest AI breakthroughs, pushing the boundaries of what’s possible in artificial intelligence. From ChatGPT to the feats of Stable diffusion, attention is the secret sauce behind these cutting-edge models. Today, we’re diving headfirst into the world of attention as used in vision. We’ll break down how it works, demystifying the ‘aha’ moment when machines learn to focus on what truly matters. Attention has been the peice which has accelerated the AI world in the recent past.We will understand each step of attention, draw out to get intuition of how it is possible to “attend” using the attention mechanism, write our attention blocks from scratch. Then we will move onto the Transformer architecture for Vision and implement the Vision Transformer (ViT). We will start with the naive application of self-attention to images that requires each pixel attends to every other pixel, then move onto Multi-head attention, and then finally go onto techniques like patching the input image as sequence when we move onto the ViT architecture.\n\n\nLet’s say we have a cat image that is of shape = (height,width,channels). Now wouldn’t it be handy if each pixel(or few pixels and/or channels together) knew about what the other part of the image looks like. If a model is working with a cat image, and if the pixels around the ear had some information or context about how the pixels at the other parts of the image are, its job would be much easier. When we apply convolutions with kernels across images to find out the activations, this information is not captured and only the local neighbouring values are learnt to create a local activation for one small part of the image. Even with the CNN receptive field this information gets faded as the network gets deeper. With attention what we are trying to do is have the weighted average of all the pixels and each pixel value is updated to be the original value and this learned weighted average of all the other pixels at different parts of the image. Naive application of self-attention to images would require that each pixel attends to every other pixel.The standard Transformer receives as input a 1D sequence of token embeddings. We flatten out our input image into (h x w, channels) dimensions. In the cells below we create an input with batch size 64, 32 channels and 16X16 pixels map and reshape it to be (n_batch, h x w, channels)\n\n\n!pip install -Uqq git+https://github.com/fastai/course22p2\n\n\n%%capture\n!pip install accelerate\n\n\nimport math,torch\nfrom torch import nn\nfrom miniai.activations import *\nfrom torch import Tensor\nfrom einops.layers.torch import Rearrange\nimport torch\nfrom torchvision.datasets import OxfordIIITPet\nimport matplotlib.pyplot as plt\nfrom random import random\nfrom torchvision.transforms import Resize, ToTensor\nfrom torchvision.transforms.functional import to_pil_image\n\n\nimport matplotlib.pyplot as plt\n\n\nset_seed(42)\nx = torch.randn(64,32,16,16)\n\n\nt = x.view(*x.shape[:2], -1).transpose(1, 2)\nt.shape\n\ntorch.Size([64, 256, 32])\n\n\n\n\nIn the original “Attention is all you need” paper it is quoted as “an attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors”. Describing it in terms of query and key-value pairs instantly makes me think of a mapping function between known vectors which is not true, as these vectors are initialised and learnt over time. Therefore, another term that is more intuitive for me to associate with q,k and v are projections. Each of the q,k and v are different projections of the input. Let’s see how these projections are created. We create 3 linear transformations with nn.Linear where in_features, out_features equal to the number of channels. The weight matrices of these 3 different transformations will be initialised randomly, and these weights(which are learnable parameters of nn.Linear) will be matrix multiplied by input which will give us the 3 different projections of the input.\n\nni = 32\nsk = nn.Linear(ni, ni)\nsq = nn.Linear(ni, ni)\nsv = nn.Linear(ni, ni)\n\n\nk = sk(t)\nq = sq(t)\nv = sv(t)\nk.shape,q.shape,v.shape\n\n(torch.Size([64, 256, 32]),\n torch.Size([64, 256, 32]),\n torch.Size([64, 256, 32]))\n\n\nLet’s get into the details and visualise these Linear Transformation. The shapes of the various components are inp,t = [64,256,32] Linear layer,l = (in_features=32,out_features=32,b=True). So the weight matrix ‘w’ of the Linear layer nn.Linear is [32,32] matrix. out,(k,q,v) = [64,256,32]\n\nNext step is to take the dot product of the q and k projections. These projections (q,k) have value of each pixel across all the channels(32). So if we take the dot product of each pixel value across all channes with another pixel value across all channels, we will get the relationship between those 2 pixels(and eventually all of the pixels in the grid) or we will get to know how similar those pixels are. Let’s visualise this for better understanding.\n\nWe do the matrix product of q with the transpose of k. We see that as we fill up the resultant 256X256 matrix, we get the relationship or similarity between each of the pixels. If we look at the first row of q and first column of k.T (selected in red color) the first pixel across channels of q projection is multiplied with first pixel across channels of k projection to fill out “a” in the resultant matrix. Similarily, as we continue the dot product we get the similarity(or relationships) between all the pixels . Let’s fill out some values in the result matrix and see what is it actually doing. So to get a,b,c,d we do the dot product of\n\n\na - 1st pixel all channels (q) X 1st pixel all channles(k)\n\n\nb - 1st pixel all channels(q) X 2nd pixel all channles(k)\n\n\nd - 2nd pixel all channels(q) X 1st pixel all channles(k)\n\n\nLet’s go ahead and do the q@k.T\n\ns = (q@k.transpose(1,2))\ns.shape\n\ntorch.Size([64, 256, 256])\n\n\nNow, we will take this result “s” and do the matrix product with the v projection and the final output of the self attention block is the original input plus all the transformations we have done using q,k,v and other projections.\n\nres = s.softmax(dim=-1)@v\n\nWe take the softmax across each row so that the sum of each row of “s” is 1 and we have a weight for each pixel.\n\nres.shape\n\ntorch.Size([64, 256, 32])\n\n\nSo now let’s write our own Self Attention class\n\nclass SelfAttention(nn.Module):\n    def __init__(self, ni):\n        super().__init__()\n        self.scale = math.sqrt(ni)\n        self.norm = nn.GroupNorm(1, ni)\n        self.q = nn.Linear(ni, ni)\n        self.k = nn.Linear(ni, ni)\n        self.v = nn.Linear(ni, ni)\n        self.proj = nn.Linear(ni, ni)\n    \n    def forward(self, x):\n        inp = x\n        n,c,h,w = x.shape\n        x = self.norm(x)\n        x = x.view(n, c, -1).transpose(1, 2)\n        q = self.q(x)\n        k = self.k(x)\n        v = self.v(x)\n        s = (q@k.transpose(1,2))/self.scale\n        x = s.softmax(dim=-1)@v\n        x = self.proj(x)\n        x = x.transpose(1,2).reshape(n,c,h,w)\n        return x+inp\n\n\nsa = SelfAttention(32)\nra = sa(x)\nra.shape\n\ntorch.Size([64, 32, 16, 16])\n\n\n\n\n\n\nNaive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. So one method which is very intuitive is to group some channels together from different parts of the image. These groups of channels are called “heads”. Now each head would have few channels grouped together and these heads will carry information about different aspects of the image depending on what the channels in a particular group(or head) capture. For understanding purposes , we can assume one head has capured part of the image which has information about the temperature(brightness) of the image, another head has information about the regions that correspond to cat features like ears, eyes, nose, or tail and so on. * As compared to our naive self attention, we do the dot product between these heads or groups of channels * As we use softmax to sum a row and get weight for each pixel, if we just use pixel self attention, the attention might be biased to just one large value as the softmax of that large value would be enormous compared to other values ( as e^x would amplify that particular value). So grouping channels together eliminates this bias.\nAs seen below, rather than doing attention on each pixel across each channel as we did in the naive self attention, we are doing attention on these heads which contain different channels grouped together. We group the channels together and carry out attention calculations.\n\nNow these heads carry different information about different parts of the image and attend to each other. Let’s see how to translate this in code. The main difference from our naive self attention is the rearrangement of the input dimensions.\n\nfrom einops import rearrange\n\n\nclass SelfAttentionMultiHead(nn.Module):\n    def __init__(self, ni, nheads):\n        super().__init__()\n        self.nheads = nheads\n        self.scale = math.sqrt(ni/nheads)\n        self.norm = nn.BatchNorm2d(ni)\n        self.qkv = nn.Linear(ni, ni*3)\n        self.proj = nn.Linear(ni, ni)\n    \n    def forward(self, inp):\n        n,c,h,w = inp.shape\n        x = self.norm(inp).view(n, c, -1).transpose(1, 2)\n        x = self.qkv(x)\n        x = rearrange(x, 'n s (h d) -&gt; (n h) s d', h=self.nheads)\n        q,k,v = torch.chunk(x, 3, dim=-1)\n        s = (q@k.transpose(1,2))/self.scale\n        x = s.softmax(dim=-1)@v\n        x = rearrange(x, '(n h) s d -&gt; n s (h d)', h=self.nheads)\n        x = self.proj(x).transpose(1,2).reshape(n,c,h,w)\n        return x+inp\n\nLet’s break down the part which is different from the naive Self Attention which is the rearrangement of the dimensions.To understand how it works, let’s try to have h=8(heads) for our multi-head attention. As our images are 32 channels, each head will have d=4(channels/head). * Now we rearrange the images of dimensions [64,256,32] or [n,s,c] into [(n h), s, d] or [512,256,4]. * What we did here was to have the 32 channels break down into 8 groups of 4. * Rather than having 64 images in each batch of 256 pixels and 32 channels, we have 512 images per batch of 256 pixels and 4 channels. * Now we decreased the number of channels for each image to 4 but we created 8 times more images in each batch. * We want each of the head(groups of channels) we created to be independent and have nothing to do with each other. So now as we turn one entire image of 32 channels into 8 images of 4 channels, they are completely independent and have nothing to do with each other as they are seperate images now. * Each of the head which has captured different set of information(channels) of the image is now independent ofhe other heads created.\n\nt = x.view(*x.shape[:2], -1).transpose(1, 2)\nt.shape\n\ntorch.Size([64, 256, 32])\n\n\n\nt = rearrange(t, 'n s (h d) -&gt; (n h) s d', h=8)\nt.shape\n\ntorch.Size([512, 256, 4])\n\n\nAfter we have done the rearrangement, we do the matrix product of the q and k.T projection as before. Then we take the softmax across the rows to get the weight pixels and finally rearrange the input images into their original projection shape with “rearrange(x, ‘(n h) s d -&gt; n s (h d)’, h=self.nheads)” and do one final projection to give it a chance to learn some additional features. Finally, we reshape the image into its original raw form(n,c,h,w) and return the input with the addition of attention weights.\n\nsa = SelfAttentionMultiHead(32, 8)\nsx = sa(x)\nsx.shape\n\ntorch.Size([64, 32, 16, 16])\n\n\n\nsx.mean(),sx.std()\n\n(tensor(-0.0306, grad_fn=&lt;MeanBackward0&gt;),\n tensor(1.0076, grad_fn=&lt;StdBackward0&gt;))\n\n\n\nt = x.view(*x.shape[:2], -1).transpose(1, 2)\nt.shape\nnm = nn.MultiheadAttention(32, num_heads=8, batch_first=True)\nnmx,nmw = nm(t,t,t)\nnmx = nmx+t\n\n\nnmx.mean(),nmx.std()\n\n(tensor(-0.0019, grad_fn=&lt;MeanBackward0&gt;),\n tensor(1.0028, grad_fn=&lt;StdBackward0&gt;))\n\n\n\n\n\nNow that we have understood what attention is in Vision, implemented naive self-attention and multihead attention let’s move onto the Vision transformer(ViT). ViT expands the success of transformer models from sequential data to images. We are going to dissect each component of the ViT model architecture for classification, implement it in code piece by piece. In the process of understanding and implementing ViT, we will touch upon few questions which can help us interpret the model. Questions like what do the attenion heads in ViT capture? Which attention heads are more important? What attention patterns have individual heads learned?\nViT classifier runs in five key steps 1. Decompose the input image into a sequence of patch tokens 2. Concatenate CLS 3. Add positional encodings 4. Multi-head self-attention 5. Use the CLS token for prediction\nBelow is the ViT architecture as drawn out in the original ViT paper\n\n\n\nOriginal Transoformers was desinged for a sequence task. Here, we transform the image into a sequence of flattened 2D patches.\n\nAs seen in the image above, we break the H,W,C image (where (H, W) is the resolution of the original image, C is the number of channels, (P, P) is the resolution of each image patch(16,16) in the above example) into N(=HW/P*P) patches. Each of the red box of each image patch represents the input patch embedding which is passed onto the ViT model as input. We just use a fully connected neurel net to get these embeddings for our implementation. The input sequence can also be formed from feature maps of a CNN which we are not doing in this implementation.\n\nto_tensor = [Resize((144, 144)), ToTensor()]\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image = t(image)\n        return image, target\n\ndef show_images(images, num_samples=40, cols=8):\n    \"\"\" Plots some samples from the dataset \"\"\"\n    plt.figure(figsize=(15,15))\n    idx = int(len(dataset) / num_samples)\n    print(images)\n    for i, img in enumerate(images):\n        if i % idx == 0:\n            plt.subplot(int(num_samples/cols) + 1, cols, int(i/idx) + 1)\n            plt.imshow(to_pil_image(img[0]))\n\n\ndataset = OxfordIIITPet(root=\".\", download=True, transforms=Compose(to_tensor))\n# show_images(dataset)\n\nLet’s breakdown one image into patches, flatten it and create its embedding by passing it onto a feed forward network\n\npatch_size=8\nin_channels = 3\nemb_size = 128\nsample_datapoint = torch.unsqueeze(dataset[0][0], 0)\nprint(\"Initial shape: \", sample_datapoint.shape)\npatch_reshaped = rearrange(sample_datapoint,'b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size)\nprint(\"Shape after einops dimensions reshape\" ,patch_reshaped.shape)\npatch_emb = nn.Linear(patch_size * patch_size * in_channels, emb_size)\npatch_emb = patch_emb(patch_reshaped)\nprint(\"Shape of the final embedded patch\", patch_emb.shape)\n\nInitial shape:  torch.Size([1, 3, 144, 144])\nShape after einops dimensions reshape torch.Size([1, 324, 192])\nShape of the final embedded patch torch.Size([1, 324, 128])\n\n\nLet’s create a class to get the Patch Embeddings\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, in_channels = 3, patch_size = 8, emb_size = 128):\n        self.patch_size = patch_size\n        super().__init__()\n        self.projection = nn.Sequential(\n            # break-down the image in s1 x s2 patches and flatten them\n            Rearrange('b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = self.projection(x)\n        return x\n\n\nsample_datapoint = torch.unsqueeze(dataset[0][0], 0)\nprint(\"Initial shape: \", sample_datapoint.shape)\nembedding = PatchEmbedding()(sample_datapoint)\nprint(\"Patches shape: \", embedding.shape)\n\nInitial shape:  torch.Size([1, 3, 144, 144])\nPatches shape:  torch.Size([1, 324, 128])\n\n\n\n\n\nTypically when you are doing a classification task, you want a all of the information you extracted into a single representation and use it for classification. When we use CNNs, we have a fully connected layer at the end of the network which acts as the classification head. As Transformer is a sequence to sequence model and as there is no decoder layer in ViT, then the length of input sequence (number of patches) equals the length of output sequence. Here we add a dummy input, call it class token and apply the classification layer on the corresponding output item. So the Cls token is a dummy input which is later filled with information collected from all of the patch input tokens. Initially it is randomly initialised and is a learnable parameter. It acts as a global feature extractor which represents the entire image and is then used for downstream tasks, which is classification in our case. CLS learns to accumulate class-related features used to generate the final class probability.\nOne interesting thing is that CLS token attends to all the other tokens and all the other tokens attend to the CLS token as well. One can think the CLS token attending to every other token will help it learn about the image information captured by the patch tokens. Do Patch tokens attending to the CLS tokens help them get some relevant information about the image class, is a something which would be interesting to explore. We will go into the different types of attention ViT captures(patch-patch,patch-cls,cls-cls) in detail later.\n\nAs we can see the red circled CLS token is concatenated with all the other positional embeddings(expained in the next section).\n\n\n\nThe zero-initialized positional encodings are added to the (1+p^2)xh matrix. They are trained to learn each patch’s positional information. Just like the CLS token, we learn these positional embeddings.\n\n\n\nThis step contains l stacked attention layers, each with n attention heads. Each head learns a (1+p2)×(1+p2) attention weight matrix A, reflecting the pair-wise attention between all 1+p^2 tokens. ( where p is the patch embedding size or patch_size).\nThe ViT transfomer encoder contains 4 building blocks. Let’s go ahead and implement them.\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n        super().__init__()\n        inner_dim = dim_head *  heads\n        project_out = not (heads == 1 and dim_head == dim)\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.norm = nn.LayerNorm(dim)\n\n        self.attend = nn.Softmax(dim = -1)\n        self.dropout = nn.Dropout(dropout)\n\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x):\n        x = self.norm(x)\n\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -&gt; b h n d', h = self.heads), qkv)\n\n        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n\n        attn = self.attend(dots)\n        attn = self.dropout(attn)\n\n        out = torch.matmul(attn, v)\n        out = rearrange(out, 'b h n d -&gt; b n (h d)')\n        return self.to_out(out)\n\n\nAttention(dim=128, heads=4, dropout=0.)(torch.ones((1, 5, 128))).shape\n\ntorch.Size([1, 5, 128])\n\n\nWe use Layer Normalisation below. If we think about why layer normalisation is used, and not the more common batch norm used in Vision.It is a common practive that layer norm is used in NLP tasks and batch norm is used in Vision tasks. As transformers were intially built for sequence nlp inputs, layer norm was used. But even in ViT, where we use transformers in vision, layer norm is still used. The reason for this might be how the normalisation stats - mean and variance, are calculated in layer norm. In batchnorm, the mean and variance statistics used for normalization are calculated across all elements of all instances in a batch(elements mean pixels and instance means an image). For layernorm, the statistics are calculated across the feature dimension, for each element and instance independently. LayerNorm computes the mean and variance of all channels at each spatial location (each pixel) independently across all images in the batch. The normalization is then performed independently for each spatial location (each pixel) using the calculated mean and variance across all channels at that location. This aligns with how transformers work. In transformers, we are not concerned with the other instances in the batch when we are doing attention, but we what is more important is how rich the feature relationship information we have captured is. So normalising across the all features(or channels) of an image is very similar to what we are doing with attention.\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\nnorm = PreNorm(128, Attention(dim=128, heads=4, dropout=0.))\nnorm(torch.ones((1, 5, 128))).shape\n\ntorch.Size([1, 5, 128])\n\n\n\nclass FeedForward(nn.Sequential):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\nff = FeedForward(dim=128, hidden_dim=256)\nff(torch.ones((1, 5, 128))).shape\n\ntorch.Size([1, 5, 128])\n\n\n\nclass ResidualAdd(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        res = x\n        x = self.fn(x, **kwargs)\n        x += res\n        return x\nresidual_att = ResidualAdd(Attention(dim=128, heads=4, dropout=0.))\nresidual_att(torch.ones((1, 5, 128))).shape\n\ntorch.Size([1, 5, 128])\n\n\n\n\n\nThis step decouples the CLS embedding from the patch tokens, and transforms it into class logits through fully-connected layers.\n\n\n\nLet’s go ahead and put all of the pieces we created together\n\nfrom einops import repeat\n\nclass ViT(nn.Module):\n    def __init__(self, ch=3, img_size=144, patch_size=4, emb_dim=32,\n                n_layers=6, out_dim=37, dropout=0.1, heads=2):\n        super(ViT, self).__init__()\n\n        # Attributes\n        self.channels = ch\n        self.height = img_size\n        self.width = img_size\n        self.patch_size = patch_size\n        self.n_layers = n_layers\n\n        # Patching\n        self.patch_embedding = PatchEmbedding(in_channels=ch,\n                                              patch_size=patch_size,\n                                              emb_size=emb_dim)\n        # Learnable params\n        num_patches = (img_size // patch_size) ** 2\n        self.pos_embedding = nn.Parameter(\n            torch.randn(1, num_patches + 1, emb_dim))\n        self.cls_token = nn.Parameter(torch.rand(1, 1, emb_dim))\n\n        # Transformer Encoder\n        self.layers = nn.ModuleList([])\n        for _ in range(n_layers):\n            transformer_block = nn.Sequential(\n                ResidualAdd(PreNorm(emb_dim, Attention(emb_dim, heads = heads, dropout = dropout))),\n                ResidualAdd(PreNorm(emb_dim, FeedForward(emb_dim, emb_dim, dropout = dropout))))\n            self.layers.append(transformer_block)\n\n        # Classification head\n        self.head = nn.Sequential(nn.LayerNorm(emb_dim), nn.Linear(emb_dim, out_dim))\n\n\n    def forward(self, img):\n        # Get patch embedding vectors\n        x = self.patch_embedding(img)\n        b, n, _ = x.shape\n\n        # Add cls token to inputs\n        cls_tokens = repeat(self.cls_token, '1 1 d -&gt; b 1 d', b = b)\n        x = torch.cat([cls_tokens, x], dim=1)\n        x += self.pos_embedding[:, :(n + 1)]\n\n        # Transformer layers\n        for i in range(self.n_layers):\n            x = self.layers[i](x)\n\n        # Output based on classification token\n        return self.head(x[:, 0, :])\n\n\nmodel = ViT()\nmodel(torch.ones((1, 3, 144, 144)))\n\ntensor([[-1.0458,  0.4171, -0.9099, -0.1380, -0.4593, -0.0648, -0.4884, -0.0094,\n          0.1941,  0.1272,  0.3649,  0.2897,  1.0110, -0.0136,  0.8620, -0.2971,\n         -0.2390, -0.0801, -0.5791, -0.2363, -0.5813, -0.1375,  0.2628,  1.3497,\n         -0.1218, -0.2292, -0.7679,  0.4300, -0.7301, -0.7377, -1.3888,  0.2043,\n         -0.5364,  0.2651,  0.2471, -0.2534,  0.1637]],\n       grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\n\n\n\nSource:How Does Attention Work in Vision Transformers? A Visual Analytics Attempt\nThe attention matrix of a head can be divided into four regions based on the source and target tokens: CLS→CLS, CLS→patches, patches→CLS, and patches→patches. All of these 4 regions represent different attention relationship and can be analysed seperately and together to see how they contribute to the results.\n\n\n\n\nWe started off by understanding the fundamentals of attention, went on to implement our own attention blocks and the ViT arechitecture. We tried to draw out each step of the process, play with the shapes of various intermediate outputs, build a basic intuition on what the attention matrices are doing and implement the entire code from scratch. As good as the results of attention in vision have been, their data hungry nature and lack of inductive bias has kept it from replacing CNNs all over. For cases where there is not enough training data CNN still might be the better option. Nonetheless, attention in vision has had a huge success and is generating state of the art results all over. Understanding how and why it works as good as it has, makes us better informed about its possibilities and limitations. With this understanding we can start using it with “attention” to its process and deduce the quality of the results better.\n\n\n\nAN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\nHow Does Attention Work in Vision Transformers? A Visual Analytics Attempt\nLucidrains implementation Github\nFast AI"
  },
  {
    "objectID": "posts/attention_in_vision/index.html#naive-self-attention",
    "href": "posts/attention_in_vision/index.html#naive-self-attention",
    "title": "Unpacking Attention & Transformers in Vision - From Theory to Implementation",
    "section": "",
    "text": "Let’s say we have a cat image that is of shape = (height,width,channels). Now wouldn’t it be handy if each pixel(or few pixels and/or channels together) knew about what the other part of the image looks like. If a model is working with a cat image, and if the pixels around the ear had some information or context about how the pixels at the other parts of the image are, its job would be much easier. When we apply convolutions with kernels across images to find out the activations, this information is not captured and only the local neighbouring values are learnt to create a local activation for one small part of the image. Even with the CNN receptive field this information gets faded as the network gets deeper. With attention what we are trying to do is have the weighted average of all the pixels and each pixel value is updated to be the original value and this learned weighted average of all the other pixels at different parts of the image. Naive application of self-attention to images would require that each pixel attends to every other pixel.The standard Transformer receives as input a 1D sequence of token embeddings. We flatten out our input image into (h x w, channels) dimensions. In the cells below we create an input with batch size 64, 32 channels and 16X16 pixels map and reshape it to be (n_batch, h x w, channels)\n\n\n!pip install -Uqq git+https://github.com/fastai/course22p2\n\n\n%%capture\n!pip install accelerate\n\n\nimport math,torch\nfrom torch import nn\nfrom miniai.activations import *\nfrom torch import Tensor\nfrom einops.layers.torch import Rearrange\nimport torch\nfrom torchvision.datasets import OxfordIIITPet\nimport matplotlib.pyplot as plt\nfrom random import random\nfrom torchvision.transforms import Resize, ToTensor\nfrom torchvision.transforms.functional import to_pil_image\n\n\nimport matplotlib.pyplot as plt\n\n\nset_seed(42)\nx = torch.randn(64,32,16,16)\n\n\nt = x.view(*x.shape[:2], -1).transpose(1, 2)\nt.shape\n\ntorch.Size([64, 256, 32])\n\n\n\n\nIn the original “Attention is all you need” paper it is quoted as “an attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors”. Describing it in terms of query and key-value pairs instantly makes me think of a mapping function between known vectors which is not true, as these vectors are initialised and learnt over time. Therefore, another term that is more intuitive for me to associate with q,k and v are projections. Each of the q,k and v are different projections of the input. Let’s see how these projections are created. We create 3 linear transformations with nn.Linear where in_features, out_features equal to the number of channels. The weight matrices of these 3 different transformations will be initialised randomly, and these weights(which are learnable parameters of nn.Linear) will be matrix multiplied by input which will give us the 3 different projections of the input.\n\nni = 32\nsk = nn.Linear(ni, ni)\nsq = nn.Linear(ni, ni)\nsv = nn.Linear(ni, ni)\n\n\nk = sk(t)\nq = sq(t)\nv = sv(t)\nk.shape,q.shape,v.shape\n\n(torch.Size([64, 256, 32]),\n torch.Size([64, 256, 32]),\n torch.Size([64, 256, 32]))\n\n\nLet’s get into the details and visualise these Linear Transformation. The shapes of the various components are inp,t = [64,256,32] Linear layer,l = (in_features=32,out_features=32,b=True). So the weight matrix ‘w’ of the Linear layer nn.Linear is [32,32] matrix. out,(k,q,v) = [64,256,32]\n\nNext step is to take the dot product of the q and k projections. These projections (q,k) have value of each pixel across all the channels(32). So if we take the dot product of each pixel value across all channes with another pixel value across all channels, we will get the relationship between those 2 pixels(and eventually all of the pixels in the grid) or we will get to know how similar those pixels are. Let’s visualise this for better understanding.\n\nWe do the matrix product of q with the transpose of k. We see that as we fill up the resultant 256X256 matrix, we get the relationship or similarity between each of the pixels. If we look at the first row of q and first column of k.T (selected in red color) the first pixel across channels of q projection is multiplied with first pixel across channels of k projection to fill out “a” in the resultant matrix. Similarily, as we continue the dot product we get the similarity(or relationships) between all the pixels . Let’s fill out some values in the result matrix and see what is it actually doing. So to get a,b,c,d we do the dot product of\n\n\na - 1st pixel all channels (q) X 1st pixel all channles(k)\n\n\nb - 1st pixel all channels(q) X 2nd pixel all channles(k)\n\n\nd - 2nd pixel all channels(q) X 1st pixel all channles(k)\n\n\nLet’s go ahead and do the q@k.T\n\ns = (q@k.transpose(1,2))\ns.shape\n\ntorch.Size([64, 256, 256])\n\n\nNow, we will take this result “s” and do the matrix product with the v projection and the final output of the self attention block is the original input plus all the transformations we have done using q,k,v and other projections.\n\nres = s.softmax(dim=-1)@v\n\nWe take the softmax across each row so that the sum of each row of “s” is 1 and we have a weight for each pixel.\n\nres.shape\n\ntorch.Size([64, 256, 32])\n\n\nSo now let’s write our own Self Attention class\n\nclass SelfAttention(nn.Module):\n    def __init__(self, ni):\n        super().__init__()\n        self.scale = math.sqrt(ni)\n        self.norm = nn.GroupNorm(1, ni)\n        self.q = nn.Linear(ni, ni)\n        self.k = nn.Linear(ni, ni)\n        self.v = nn.Linear(ni, ni)\n        self.proj = nn.Linear(ni, ni)\n    \n    def forward(self, x):\n        inp = x\n        n,c,h,w = x.shape\n        x = self.norm(x)\n        x = x.view(n, c, -1).transpose(1, 2)\n        q = self.q(x)\n        k = self.k(x)\n        v = self.v(x)\n        s = (q@k.transpose(1,2))/self.scale\n        x = s.softmax(dim=-1)@v\n        x = self.proj(x)\n        x = x.transpose(1,2).reshape(n,c,h,w)\n        return x+inp\n\n\nsa = SelfAttention(32)\nra = sa(x)\nra.shape\n\ntorch.Size([64, 32, 16, 16])"
  },
  {
    "objectID": "posts/attention_in_vision/index.html#multihead-self-attention",
    "href": "posts/attention_in_vision/index.html#multihead-self-attention",
    "title": "Unpacking Attention & Transformers in Vision - From Theory to Implementation",
    "section": "",
    "text": "Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. So one method which is very intuitive is to group some channels together from different parts of the image. These groups of channels are called “heads”. Now each head would have few channels grouped together and these heads will carry information about different aspects of the image depending on what the channels in a particular group(or head) capture. For understanding purposes , we can assume one head has capured part of the image which has information about the temperature(brightness) of the image, another head has information about the regions that correspond to cat features like ears, eyes, nose, or tail and so on. * As compared to our naive self attention, we do the dot product between these heads or groups of channels * As we use softmax to sum a row and get weight for each pixel, if we just use pixel self attention, the attention might be biased to just one large value as the softmax of that large value would be enormous compared to other values ( as e^x would amplify that particular value). So grouping channels together eliminates this bias.\nAs seen below, rather than doing attention on each pixel across each channel as we did in the naive self attention, we are doing attention on these heads which contain different channels grouped together. We group the channels together and carry out attention calculations.\n\nNow these heads carry different information about different parts of the image and attend to each other. Let’s see how to translate this in code. The main difference from our naive self attention is the rearrangement of the input dimensions.\n\nfrom einops import rearrange\n\n\nclass SelfAttentionMultiHead(nn.Module):\n    def __init__(self, ni, nheads):\n        super().__init__()\n        self.nheads = nheads\n        self.scale = math.sqrt(ni/nheads)\n        self.norm = nn.BatchNorm2d(ni)\n        self.qkv = nn.Linear(ni, ni*3)\n        self.proj = nn.Linear(ni, ni)\n    \n    def forward(self, inp):\n        n,c,h,w = inp.shape\n        x = self.norm(inp).view(n, c, -1).transpose(1, 2)\n        x = self.qkv(x)\n        x = rearrange(x, 'n s (h d) -&gt; (n h) s d', h=self.nheads)\n        q,k,v = torch.chunk(x, 3, dim=-1)\n        s = (q@k.transpose(1,2))/self.scale\n        x = s.softmax(dim=-1)@v\n        x = rearrange(x, '(n h) s d -&gt; n s (h d)', h=self.nheads)\n        x = self.proj(x).transpose(1,2).reshape(n,c,h,w)\n        return x+inp\n\nLet’s break down the part which is different from the naive Self Attention which is the rearrangement of the dimensions.To understand how it works, let’s try to have h=8(heads) for our multi-head attention. As our images are 32 channels, each head will have d=4(channels/head). * Now we rearrange the images of dimensions [64,256,32] or [n,s,c] into [(n h), s, d] or [512,256,4]. * What we did here was to have the 32 channels break down into 8 groups of 4. * Rather than having 64 images in each batch of 256 pixels and 32 channels, we have 512 images per batch of 256 pixels and 4 channels. * Now we decreased the number of channels for each image to 4 but we created 8 times more images in each batch. * We want each of the head(groups of channels) we created to be independent and have nothing to do with each other. So now as we turn one entire image of 32 channels into 8 images of 4 channels, they are completely independent and have nothing to do with each other as they are seperate images now. * Each of the head which has captured different set of information(channels) of the image is now independent ofhe other heads created.\n\nt = x.view(*x.shape[:2], -1).transpose(1, 2)\nt.shape\n\ntorch.Size([64, 256, 32])\n\n\n\nt = rearrange(t, 'n s (h d) -&gt; (n h) s d', h=8)\nt.shape\n\ntorch.Size([512, 256, 4])\n\n\nAfter we have done the rearrangement, we do the matrix product of the q and k.T projection as before. Then we take the softmax across the rows to get the weight pixels and finally rearrange the input images into their original projection shape with “rearrange(x, ‘(n h) s d -&gt; n s (h d)’, h=self.nheads)” and do one final projection to give it a chance to learn some additional features. Finally, we reshape the image into its original raw form(n,c,h,w) and return the input with the addition of attention weights.\n\nsa = SelfAttentionMultiHead(32, 8)\nsx = sa(x)\nsx.shape\n\ntorch.Size([64, 32, 16, 16])\n\n\n\nsx.mean(),sx.std()\n\n(tensor(-0.0306, grad_fn=&lt;MeanBackward0&gt;),\n tensor(1.0076, grad_fn=&lt;StdBackward0&gt;))\n\n\n\nt = x.view(*x.shape[:2], -1).transpose(1, 2)\nt.shape\nnm = nn.MultiheadAttention(32, num_heads=8, batch_first=True)\nnmx,nmw = nm(t,t,t)\nnmx = nmx+t\n\n\nnmx.mean(),nmx.std()\n\n(tensor(-0.0019, grad_fn=&lt;MeanBackward0&gt;),\n tensor(1.0028, grad_fn=&lt;StdBackward0&gt;))"
  },
  {
    "objectID": "posts/attention_in_vision/index.html#vision-transformers-vit",
    "href": "posts/attention_in_vision/index.html#vision-transformers-vit",
    "title": "Unpacking Attention & Transformers in Vision - From Theory to Implementation",
    "section": "",
    "text": "Now that we have understood what attention is in Vision, implemented naive self-attention and multihead attention let’s move onto the Vision transformer(ViT). ViT expands the success of transformer models from sequential data to images. We are going to dissect each component of the ViT model architecture for classification, implement it in code piece by piece. In the process of understanding and implementing ViT, we will touch upon few questions which can help us interpret the model. Questions like what do the attenion heads in ViT capture? Which attention heads are more important? What attention patterns have individual heads learned?\nViT classifier runs in five key steps 1. Decompose the input image into a sequence of patch tokens 2. Concatenate CLS 3. Add positional encodings 4. Multi-head self-attention 5. Use the CLS token for prediction\nBelow is the ViT architecture as drawn out in the original ViT paper\n\n\n\nOriginal Transoformers was desinged for a sequence task. Here, we transform the image into a sequence of flattened 2D patches.\n\nAs seen in the image above, we break the H,W,C image (where (H, W) is the resolution of the original image, C is the number of channels, (P, P) is the resolution of each image patch(16,16) in the above example) into N(=HW/P*P) patches. Each of the red box of each image patch represents the input patch embedding which is passed onto the ViT model as input. We just use a fully connected neurel net to get these embeddings for our implementation. The input sequence can also be formed from feature maps of a CNN which we are not doing in this implementation.\n\nto_tensor = [Resize((144, 144)), ToTensor()]\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image = t(image)\n        return image, target\n\ndef show_images(images, num_samples=40, cols=8):\n    \"\"\" Plots some samples from the dataset \"\"\"\n    plt.figure(figsize=(15,15))\n    idx = int(len(dataset) / num_samples)\n    print(images)\n    for i, img in enumerate(images):\n        if i % idx == 0:\n            plt.subplot(int(num_samples/cols) + 1, cols, int(i/idx) + 1)\n            plt.imshow(to_pil_image(img[0]))\n\n\ndataset = OxfordIIITPet(root=\".\", download=True, transforms=Compose(to_tensor))\n# show_images(dataset)\n\nLet’s breakdown one image into patches, flatten it and create its embedding by passing it onto a feed forward network\n\npatch_size=8\nin_channels = 3\nemb_size = 128\nsample_datapoint = torch.unsqueeze(dataset[0][0], 0)\nprint(\"Initial shape: \", sample_datapoint.shape)\npatch_reshaped = rearrange(sample_datapoint,'b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size)\nprint(\"Shape after einops dimensions reshape\" ,patch_reshaped.shape)\npatch_emb = nn.Linear(patch_size * patch_size * in_channels, emb_size)\npatch_emb = patch_emb(patch_reshaped)\nprint(\"Shape of the final embedded patch\", patch_emb.shape)\n\nInitial shape:  torch.Size([1, 3, 144, 144])\nShape after einops dimensions reshape torch.Size([1, 324, 192])\nShape of the final embedded patch torch.Size([1, 324, 128])\n\n\nLet’s create a class to get the Patch Embeddings\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, in_channels = 3, patch_size = 8, emb_size = 128):\n        self.patch_size = patch_size\n        super().__init__()\n        self.projection = nn.Sequential(\n            # break-down the image in s1 x s2 patches and flatten them\n            Rearrange('b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n        )\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = self.projection(x)\n        return x\n\n\nsample_datapoint = torch.unsqueeze(dataset[0][0], 0)\nprint(\"Initial shape: \", sample_datapoint.shape)\nembedding = PatchEmbedding()(sample_datapoint)\nprint(\"Patches shape: \", embedding.shape)\n\nInitial shape:  torch.Size([1, 3, 144, 144])\nPatches shape:  torch.Size([1, 324, 128])\n\n\n\n\n\nTypically when you are doing a classification task, you want a all of the information you extracted into a single representation and use it for classification. When we use CNNs, we have a fully connected layer at the end of the network which acts as the classification head. As Transformer is a sequence to sequence model and as there is no decoder layer in ViT, then the length of input sequence (number of patches) equals the length of output sequence. Here we add a dummy input, call it class token and apply the classification layer on the corresponding output item. So the Cls token is a dummy input which is later filled with information collected from all of the patch input tokens. Initially it is randomly initialised and is a learnable parameter. It acts as a global feature extractor which represents the entire image and is then used for downstream tasks, which is classification in our case. CLS learns to accumulate class-related features used to generate the final class probability.\nOne interesting thing is that CLS token attends to all the other tokens and all the other tokens attend to the CLS token as well. One can think the CLS token attending to every other token will help it learn about the image information captured by the patch tokens. Do Patch tokens attending to the CLS tokens help them get some relevant information about the image class, is a something which would be interesting to explore. We will go into the different types of attention ViT captures(patch-patch,patch-cls,cls-cls) in detail later.\n\nAs we can see the red circled CLS token is concatenated with all the other positional embeddings(expained in the next section).\n\n\n\nThe zero-initialized positional encodings are added to the (1+p^2)xh matrix. They are trained to learn each patch’s positional information. Just like the CLS token, we learn these positional embeddings.\n\n\n\nThis step contains l stacked attention layers, each with n attention heads. Each head learns a (1+p2)×(1+p2) attention weight matrix A, reflecting the pair-wise attention between all 1+p^2 tokens. ( where p is the patch embedding size or patch_size).\nThe ViT transfomer encoder contains 4 building blocks. Let’s go ahead and implement them.\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n        super().__init__()\n        inner_dim = dim_head *  heads\n        project_out = not (heads == 1 and dim_head == dim)\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.norm = nn.LayerNorm(dim)\n\n        self.attend = nn.Softmax(dim = -1)\n        self.dropout = nn.Dropout(dropout)\n\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x):\n        x = self.norm(x)\n\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -&gt; b h n d', h = self.heads), qkv)\n\n        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n\n        attn = self.attend(dots)\n        attn = self.dropout(attn)\n\n        out = torch.matmul(attn, v)\n        out = rearrange(out, 'b h n d -&gt; b n (h d)')\n        return self.to_out(out)\n\n\nAttention(dim=128, heads=4, dropout=0.)(torch.ones((1, 5, 128))).shape\n\ntorch.Size([1, 5, 128])\n\n\nWe use Layer Normalisation below. If we think about why layer normalisation is used, and not the more common batch norm used in Vision.It is a common practive that layer norm is used in NLP tasks and batch norm is used in Vision tasks. As transformers were intially built for sequence nlp inputs, layer norm was used. But even in ViT, where we use transformers in vision, layer norm is still used. The reason for this might be how the normalisation stats - mean and variance, are calculated in layer norm. In batchnorm, the mean and variance statistics used for normalization are calculated across all elements of all instances in a batch(elements mean pixels and instance means an image). For layernorm, the statistics are calculated across the feature dimension, for each element and instance independently. LayerNorm computes the mean and variance of all channels at each spatial location (each pixel) independently across all images in the batch. The normalization is then performed independently for each spatial location (each pixel) using the calculated mean and variance across all channels at that location. This aligns with how transformers work. In transformers, we are not concerned with the other instances in the batch when we are doing attention, but we what is more important is how rich the feature relationship information we have captured is. So normalising across the all features(or channels) of an image is very similar to what we are doing with attention.\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\nnorm = PreNorm(128, Attention(dim=128, heads=4, dropout=0.))\nnorm(torch.ones((1, 5, 128))).shape\n\ntorch.Size([1, 5, 128])\n\n\n\nclass FeedForward(nn.Sequential):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\nff = FeedForward(dim=128, hidden_dim=256)\nff(torch.ones((1, 5, 128))).shape\n\ntorch.Size([1, 5, 128])\n\n\n\nclass ResidualAdd(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        res = x\n        x = self.fn(x, **kwargs)\n        x += res\n        return x\nresidual_att = ResidualAdd(Attention(dim=128, heads=4, dropout=0.))\nresidual_att(torch.ones((1, 5, 128))).shape\n\ntorch.Size([1, 5, 128])\n\n\n\n\n\nThis step decouples the CLS embedding from the patch tokens, and transforms it into class logits through fully-connected layers.\n\n\n\nLet’s go ahead and put all of the pieces we created together\n\nfrom einops import repeat\n\nclass ViT(nn.Module):\n    def __init__(self, ch=3, img_size=144, patch_size=4, emb_dim=32,\n                n_layers=6, out_dim=37, dropout=0.1, heads=2):\n        super(ViT, self).__init__()\n\n        # Attributes\n        self.channels = ch\n        self.height = img_size\n        self.width = img_size\n        self.patch_size = patch_size\n        self.n_layers = n_layers\n\n        # Patching\n        self.patch_embedding = PatchEmbedding(in_channels=ch,\n                                              patch_size=patch_size,\n                                              emb_size=emb_dim)\n        # Learnable params\n        num_patches = (img_size // patch_size) ** 2\n        self.pos_embedding = nn.Parameter(\n            torch.randn(1, num_patches + 1, emb_dim))\n        self.cls_token = nn.Parameter(torch.rand(1, 1, emb_dim))\n\n        # Transformer Encoder\n        self.layers = nn.ModuleList([])\n        for _ in range(n_layers):\n            transformer_block = nn.Sequential(\n                ResidualAdd(PreNorm(emb_dim, Attention(emb_dim, heads = heads, dropout = dropout))),\n                ResidualAdd(PreNorm(emb_dim, FeedForward(emb_dim, emb_dim, dropout = dropout))))\n            self.layers.append(transformer_block)\n\n        # Classification head\n        self.head = nn.Sequential(nn.LayerNorm(emb_dim), nn.Linear(emb_dim, out_dim))\n\n\n    def forward(self, img):\n        # Get patch embedding vectors\n        x = self.patch_embedding(img)\n        b, n, _ = x.shape\n\n        # Add cls token to inputs\n        cls_tokens = repeat(self.cls_token, '1 1 d -&gt; b 1 d', b = b)\n        x = torch.cat([cls_tokens, x], dim=1)\n        x += self.pos_embedding[:, :(n + 1)]\n\n        # Transformer layers\n        for i in range(self.n_layers):\n            x = self.layers[i](x)\n\n        # Output based on classification token\n        return self.head(x[:, 0, :])\n\n\nmodel = ViT()\nmodel(torch.ones((1, 3, 144, 144)))\n\ntensor([[-1.0458,  0.4171, -0.9099, -0.1380, -0.4593, -0.0648, -0.4884, -0.0094,\n          0.1941,  0.1272,  0.3649,  0.2897,  1.0110, -0.0136,  0.8620, -0.2971,\n         -0.2390, -0.0801, -0.5791, -0.2363, -0.5813, -0.1375,  0.2628,  1.3497,\n         -0.1218, -0.2292, -0.7679,  0.4300, -0.7301, -0.7377, -1.3888,  0.2043,\n         -0.5364,  0.2651,  0.2471, -0.2534,  0.1637]],\n       grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\n\n\n\nSource:How Does Attention Work in Vision Transformers? A Visual Analytics Attempt\nThe attention matrix of a head can be divided into four regions based on the source and target tokens: CLS→CLS, CLS→patches, patches→CLS, and patches→patches. All of these 4 regions represent different attention relationship and can be analysed seperately and together to see how they contribute to the results."
  },
  {
    "objectID": "posts/attention_in_vision/index.html#conclusion",
    "href": "posts/attention_in_vision/index.html#conclusion",
    "title": "Unpacking Attention & Transformers in Vision - From Theory to Implementation",
    "section": "",
    "text": "We started off by understanding the fundamentals of attention, went on to implement our own attention blocks and the ViT arechitecture. We tried to draw out each step of the process, play with the shapes of various intermediate outputs, build a basic intuition on what the attention matrices are doing and implement the entire code from scratch. As good as the results of attention in vision have been, their data hungry nature and lack of inductive bias has kept it from replacing CNNs all over. For cases where there is not enough training data CNN still might be the better option. Nonetheless, attention in vision has had a huge success and is generating state of the art results all over. Understanding how and why it works as good as it has, makes us better informed about its possibilities and limitations. With this understanding we can start using it with “attention” to its process and deduce the quality of the results better.\n\n\n\nAN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\nHow Does Attention Work in Vision Transformers? A Visual Analytics Attempt\nLucidrains implementation Github\nFast AI"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Sehaj, and this blog is my personal exploration into the depths of artificial intelligence and machine learning. Here, I share my insights, reflections, and opinions on the latest technologies, models, and papers that capture my curiosity."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sehaj-notepad",
    "section": "",
    "text": "Unpacking Attention & Transformers in Vision - From Theory to Implementation\n\n\n\n\n\n\nAI\n\n\nAttention\n\n\nTransformers\n\n\nVision\n\n\npytorch\n\n\njupyter\n\n\ncode\n\n\n\nAttention in Vision explained.\n\n\n\n\n\nApr 6, 2024\n\n\nSehajdeep Singh\n\n\n\n\n\n\n\n\n\n\n\n\nLatent Diffusion and Perceptual Latent Loss to Generate Church images\n\n\n\n\n\n\nAI\n\n\nLatent Diffusion\n\n\npython\n\n\njupyter\n\n\ncode\n\n\n\nLatent Diffusion to generate Church images.\n\n\n\n\n\nMar 20, 2024\n\n\nSehajdeep Singh\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Cellular Automata\n\n\n\n\n\n\nAI\n\n\npython\n\n\njupyter\n\n\ncode\n\n\n\nSelf-organizing systems of simple cells can produce complex patterns.\n\n\n\n\n\nFeb 5, 2024\n\n\nSehajdeep Singh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/latent_diffusion/index.html",
    "href": "posts/latent_diffusion/index.html",
    "title": "Latent Diffusion and Perceptual Latent Loss to Generate Church images",
    "section": "",
    "text": "In today’s post, we will build Latent Diffusion models to generate Church images at reduced training times using latents and using a pre-trained ImageNet latent classifier as a component to add perceptual loss. We will be using LSUN Church dataset, trained for 30 epochs on our U-Net model. We will first train a U-Net model with just MSE loss to sample our Church images, and then intorduce the a perceptual loss function in the mix to see how it affects the generated samples.The intuition behind using the Imagenet latent classifier to add perceptual loss is that the model has learnt about images in the latent space and its parameters have captured information about features of images, all in the compressed latent space.By focusing on high-level features, perceptual loss function can produce results that align better with human visual perception, leading to higher-quality image generation. Let’s dive into the notebook and understand the entire process piece by piece.\nThe notebook is structured as follows - We compress(encode) the LSUN Church dataset using sd-vae-ft-ema VAE.These latents map 3 channel 256 x 256 pixel images down by a spatial resolution factor of 8 to 4 channels 32 x 32 latents. - Then we add noise to these latent images using Linear Noise scheduler and train a U-Net model to predict noise in an image which is in the latent space using these encoded latents. First the model is trained just with MSE loss, then perceptual loss is added to create a combined loss function. The perceptual loss is added courtesy of a external network trained on the entire Imagenet Dataset in the latent space. - Using DDIM sampler, we generate Church images.\n\n\n\n!pip install -Uqq git+https://github.com/fastai/course22p2\n\n\n%%capture\n!pip install git+https://github.com/huggingface/transformers\n\n\n%%capture\n!pip install accelerate\n\n\nimport timm, torch, random, datasets, math, fastcore.all as fc, numpy as np, matplotlib as mpl, matplotlib.pyplot as plt\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader,default_collate\nfrom pathlib import Path\nfrom torch.nn import init\nfrom fastcore.foundation import L\nfrom torch import nn,tensor\nfrom datasets import load_dataset\nfrom operator import itemgetter\nfrom torcheval.metrics import MulticlassAccuracy\nfrom functools import partial\nfrom torch.optim import lr_scheduler\nfrom torch import optim\nfrom torchvision.io import read_image,ImageReadMode\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.init import *\nfrom miniai.sgd import *\n# from miniai.resnet import *\nfrom miniai.augment import *\nfrom miniai.accel import *\nfrom miniai.training import *\n\n\n# from miniai.imports import *\nfrom miniai.diffusion import *\n\nfrom glob import glob\nfrom fastprogress import progress_bar\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\n\n\nimport timm\n\n\ntorch.set_printoptions(precision=4, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray_r'\nmpl.rcParams['figure.dpi'] = 70\n\nset_seed(42)\nif fc.defaults.cpus&gt;8: fc.defaults.cpus=8\n\n\n\n\nWe will start off by compressing the LSUN Church dataset images into VAE encoded latents.\n\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath = path_data/'church'\n\n\nbs = 64\n\n\ndef to_img(f): return read_image(f, mode=ImageReadMode.RGB)/255\n\nWe read each file and convert it into a 256X256 image. Remember, VAE encodes 256X256 images into 32X32 in the latent space(and 64X64 for 512X512 images). We do need the images to be of same size to pass it to VAE decoder in batches.\n\nclass ImagesDS:\n    def __init__(self, spec):\n        self.path = Path(path)\n        self.files = glob(str(spec), recursive=True)\n    def __len__(self): return len(self.files)\n    def __getitem__(self, i): return to_img(self.files[i])[:, :256,:256]\n\n\nds = ImagesDS(path/f'**/*.jpeg')\n\n\nlen(ds)\n\n126227\n\n\nLet’s have a look into our dataset\n\ndl = DataLoader(ds, batch_size=bs, num_workers=fc.defaults.cpus)\nxb = next(iter(dl))\nshow_images(xb[:16], imsize=3)\n\n\n\n\n\n\n\n\n\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\").cuda().requires_grad_(False)\n\n\n\n\n\n\n\n\nxe = vae.encode(xb.cuda())\n\n\nxs = xe.latent_dist.mean[:16]\nxs.shape\n\ntorch.Size([16, 4, 32, 32])\n\n\nThe below cell shows that the encoded images are 48 times smaller than the original pixel images requiring 48 times less memory and less compute.\n\n(16*3*256*256)/(16*4*32*32)\n\n48.0\n\n\nLet’s visualise the images in the latent space\n\nshow_images(((xs[:16,:3])/4).sigmoid(), imsize=2)\n\n\n\n\n\n\n\n\nThe smaller encoded images are no good, if they can’t be transformed into their original form without losing its characteristics and visuals. We decode them using VAE and they look as good as the originals.\n\nxd = to_cpu(vae.decode(xs))\nshow_images(xd['sample'].clamp(0,1), imsize=3)\n\n\n\n\n\n\n\n\n\n\nWe are going to store our latent data into a memory mapped numpy file. Whatever memory numpy uses in RAM, is copied to the disk and all operations are written onto disk as well.Even though it is on disk, it still uses caching to get the data on RAM which you are using at a particular instance without comprimising speed.\n\nmmpath = Path('data/church/data.npmm')\n\nThe first dimension is the number of images in our dataset\n\nmmshape = (126227,4,32,32)\n\nWe go through the dataset, encode it as a (4,32,32) latent image, and save it on disk.\n\nif not mmpath.exists():\n    a = np.memmap(mmpath, np.float32, mode='w+', shape=mmshape)\n    i = 0\n    for b in progress_bar(dl):\n        n = len(b)\n        a[i:i+n] = to_cpu(vae.encode(b.cuda()).latent_dist.mean).numpy()\n        i += n\n    a.flush()\n    del(a)\n\n\n\n\n\n\n    \n      \n      100.00% [1973/1973 49:27&lt;00:00]\n    \n    \n\n\n\nlats = np.memmap(mmpath, dtype=np.float32, mode='r', shape=mmshape)\n\n\nb = torch.tensor(lats[:16])\n\n\nxd = to_cpu(vae.decode(b.cuda()))\nshow_images(xd['sample'].clamp(0,1), imsize=2)"
  },
  {
    "objectID": "posts/latent_diffusion/index.html#imports",
    "href": "posts/latent_diffusion/index.html#imports",
    "title": "Latent Diffusion and Perceptual Latent Loss to Generate Church images",
    "section": "",
    "text": "!pip install -Uqq git+https://github.com/fastai/course22p2\n\n\n%%capture\n!pip install git+https://github.com/huggingface/transformers\n\n\n%%capture\n!pip install accelerate\n\n\nimport timm, torch, random, datasets, math, fastcore.all as fc, numpy as np, matplotlib as mpl, matplotlib.pyplot as plt\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader,default_collate\nfrom pathlib import Path\nfrom torch.nn import init\nfrom fastcore.foundation import L\nfrom torch import nn,tensor\nfrom datasets import load_dataset\nfrom operator import itemgetter\nfrom torcheval.metrics import MulticlassAccuracy\nfrom functools import partial\nfrom torch.optim import lr_scheduler\nfrom torch import optim\nfrom torchvision.io import read_image,ImageReadMode\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.init import *\nfrom miniai.sgd import *\n# from miniai.resnet import *\nfrom miniai.augment import *\nfrom miniai.accel import *\nfrom miniai.training import *\n\n\n# from miniai.imports import *\nfrom miniai.diffusion import *\n\nfrom glob import glob\nfrom fastprogress import progress_bar\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\n\n\nimport timm\n\n\ntorch.set_printoptions(precision=4, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray_r'\nmpl.rcParams['figure.dpi'] = 70\n\nset_seed(42)\nif fc.defaults.cpus&gt;8: fc.defaults.cpus=8"
  },
  {
    "objectID": "posts/latent_diffusion/index.html#data",
    "href": "posts/latent_diffusion/index.html#data",
    "title": "Latent Diffusion and Perceptual Latent Loss to Generate Church images",
    "section": "",
    "text": "We will start off by compressing the LSUN Church dataset images into VAE encoded latents.\n\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath = path_data/'church'\n\n\nbs = 64\n\n\ndef to_img(f): return read_image(f, mode=ImageReadMode.RGB)/255\n\nWe read each file and convert it into a 256X256 image. Remember, VAE encodes 256X256 images into 32X32 in the latent space(and 64X64 for 512X512 images). We do need the images to be of same size to pass it to VAE decoder in batches.\n\nclass ImagesDS:\n    def __init__(self, spec):\n        self.path = Path(path)\n        self.files = glob(str(spec), recursive=True)\n    def __len__(self): return len(self.files)\n    def __getitem__(self, i): return to_img(self.files[i])[:, :256,:256]\n\n\nds = ImagesDS(path/f'**/*.jpeg')\n\n\nlen(ds)\n\n126227\n\n\nLet’s have a look into our dataset\n\ndl = DataLoader(ds, batch_size=bs, num_workers=fc.defaults.cpus)\nxb = next(iter(dl))\nshow_images(xb[:16], imsize=3)\n\n\n\n\n\n\n\n\n\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\").cuda().requires_grad_(False)\n\n\n\n\n\n\n\n\nxe = vae.encode(xb.cuda())\n\n\nxs = xe.latent_dist.mean[:16]\nxs.shape\n\ntorch.Size([16, 4, 32, 32])\n\n\nThe below cell shows that the encoded images are 48 times smaller than the original pixel images requiring 48 times less memory and less compute.\n\n(16*3*256*256)/(16*4*32*32)\n\n48.0\n\n\nLet’s visualise the images in the latent space\n\nshow_images(((xs[:16,:3])/4).sigmoid(), imsize=2)\n\n\n\n\n\n\n\n\nThe smaller encoded images are no good, if they can’t be transformed into their original form without losing its characteristics and visuals. We decode them using VAE and they look as good as the originals.\n\nxd = to_cpu(vae.decode(xs))\nshow_images(xd['sample'].clamp(0,1), imsize=3)\n\n\n\n\n\n\n\n\n\n\nWe are going to store our latent data into a memory mapped numpy file. Whatever memory numpy uses in RAM, is copied to the disk and all operations are written onto disk as well.Even though it is on disk, it still uses caching to get the data on RAM which you are using at a particular instance without comprimising speed.\n\nmmpath = Path('data/church/data.npmm')\n\nThe first dimension is the number of images in our dataset\n\nmmshape = (126227,4,32,32)\n\nWe go through the dataset, encode it as a (4,32,32) latent image, and save it on disk.\n\nif not mmpath.exists():\n    a = np.memmap(mmpath, np.float32, mode='w+', shape=mmshape)\n    i = 0\n    for b in progress_bar(dl):\n        n = len(b)\n        a[i:i+n] = to_cpu(vae.encode(b.cuda()).latent_dist.mean).numpy()\n        i += n\n    a.flush()\n    del(a)\n\n\n\n\n\n\n    \n      \n      100.00% [1973/1973 49:27&lt;00:00]\n    \n    \n\n\n\nlats = np.memmap(mmpath, dtype=np.float32, mode='r', shape=mmshape)\n\n\nb = torch.tensor(lats[:16])\n\n\nxd = to_cpu(vae.decode(b.cuda()))\nshow_images(xd['sample'].clamp(0,1), imsize=2)"
  },
  {
    "objectID": "posts/latent_diffusion/index.html#analzying-the-loss",
    "href": "posts/latent_diffusion/index.html#analzying-the-loss",
    "title": "Latent Diffusion and Perceptual Latent Loss to Generate Church images",
    "section": "Analzying the loss",
    "text": "Analzying the loss\nThe model is trained for a total of 30 epochs. One thing that catches the eye is the loss value being high. If we do Simple diffusion in pixel space, loss is much lower(into 0.03 after 15 epochs).The reason for the loss being high is the complexity of pixel generation in the latent space. - Pixel space values directly represent the visual appearance of images and are highly correlated with features such as shapes and textures, where as latent space values provide a more interpretable and lower-dimensional representation of the underlying features and attributes of the data. - So generating image pixels are much easier as the nearby pixels will have similar values, the background might be same. When you try to generate latent pixels, the task is much more precise about what you want to generate and the error goes up as it is a much more difficult task for the model"
  },
  {
    "objectID": "posts/latent_diffusion/index.html#ddim-sampler",
    "href": "posts/latent_diffusion/index.html#ddim-sampler",
    "title": "Latent Diffusion and Perceptual Latent Loss to Generate Church images",
    "section": "DDIM Sampler",
    "text": "DDIM Sampler\n\n\nA sampling step in a diffusion model consists of: - predicting the direction in input space in which we should move to remove noise, or equivalently, to make the input more likely under the data distribution; - taking a small step in that direction.\nThe above image shows the main equations that make up the DDIM paper. The σ(sigma) in the above equations is what makes this algorithm deterministic(and the difference from the probalistic predecessor DDPM). As we run this DDIM algorithm once, we get a direction towards the target(input) data distribution and take a step towards that direction.This direction is just an estimate of where our actual data distriution is and is not necessarily the absolute right direction. If we proceed to take a step in this direction and add some noise (as we do in the DDPM sampling algorithm, for example), we end up with \\(x_{t-1}\\),which corresponds to a slightly less noisy input image. The predicted direction now points to a smaller, “more specific” region of high likelihood, because some uncertainty was resolved by the previous sampling step. We add the noise back to this predicted direction so that the model can explore different regions of the data distribution.\nIn DDIM, the σ(sigma) controls how muchh noise we add which makes the process deterministic. As we can see from the second equation in the above image, the “random noise” σ\\(_{t}\\)ε\\(_{t}\\) is added back. If σ is zero the algorithm becomes completely deterministic and if σ is 1 it is just DDPM. Despite the stochastic nature of adding noise during sampling, the overall sampling process in DDIM can still be considered deterministic in the sense that given the same input (noisy observation) and the same diffusion parameters, the output (sampled image) will be consistent and reproducible.\n\ncheckpoint = torch.load(\"models/church_mseonly\")\n\n\nmodel = EmbUNetModel(in_channels=4, out_channels=4, nfs=(128, 256, 512, 768), num_layers=2,\n                     attn_start=1, attn_chans=16)\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\n&lt;All keys matched successfully&gt;\n\n\n\nmodel = model.cuda() \n\n\nsz = (16,4,32,32)\n\n\nmodel = model.cuda()\n# set_seed(42)\npreds = sample(ddim_step, model, sz, steps=100, eta=1., clamp=False)\n\n\n\n\n\n\n    \n      \n      100.00% [100/100 00:02&lt;00:00]\n    \n    \n\n\n\ns = preds[-1]*5\n\n\nwith torch.no_grad(): pd = to_cpu(vae.decode(s.cuda()))\n\nThe samples generated were able to capture the nuances of the scene including the main Church, the background and foreground settings. In some of the generated samples it was able to capture details of humans,or human crowds in the foreground. This is quite remarkable for a model trained just for 3-4 hours on a A1000 gpu card from scratch.\n\nshow_images(pd['sample'][:16].clamp(0,1), imsize=5)\n\n\n\n\n\n\n\n\n## Humans in the foreground\nLet’s look at this particular sample, where we can see the entire scene of a Church in front of a lake, where boat like structures, flowers and humans on the edge of the boat on the land seem to appear. We can see what looks like a crowd of few people, though the feature details seem to be missing. Nevertheless, the amount of object detail it has captured in the sampling is a huge positive.\n\nshow_images(pd['sample'][9:10].clamp(0,1), imsize=5)\n\n\n\n\n\n\n\n\n\nshow_images(pd['sample'][:16].clamp(0,1), imsize=5)\n\n\n\n\n\n\n\n\n\nshow_images(pd['sample'][:45].clamp(0,1), imsize=5)"
  },
  {
    "objectID": "posts/latent_diffusion/index.html#conclusion",
    "href": "posts/latent_diffusion/index.html#conclusion",
    "title": "Latent Diffusion and Perceptual Latent Loss to Generate Church images",
    "section": "Conclusion",
    "text": "Conclusion\nToday, we generated Church images by training 2 diffusion models, one with MSE loss and the other with added perceptual loss. The pieces we scraped together to generate images and train these models were our very own. Considering the dataset we used was very small(for a generative model trained from scratch), and each of the model trained for just 3-4 hours on a 16Gb A1000 card, we got promising results.\nAnother way to add perceptual loss is to treat the diffusion model itself as the network which will generate meaningful perceptual loss. This technique claims to have generated better unconditional samples against models trained with just MSE loss."
  }
]