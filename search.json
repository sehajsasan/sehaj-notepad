[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/nca/index.html",
    "href": "posts/nca/index.html",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "What we are going to do is to try to create a diffrentiable self-organising system Neural Cellular Automata with just a few hundred parameters.We set up a system of ‘cells’, often represented by pixels in a line or a grid. Each cell can ‘see’ their immediate neighbors, and can change it’s output based on this information. We ll use Cellular Automata models to identifying cell-level rules which give rise to complex, regenerative behavior of the collective.\nCellular Automatas consist of a grid of cells being iteratively updated, with the same set of rules being applied to each cell at every step.The new state of a cell depends only on the states of the few cells in its immediate neighborhood.\nSo to put it in simple words, we start out with a random grid with pure noise pixels. Each cell in the grid only knows about the states of its immediate neighbourhood cells.With this structure in place, we will produce a predefined multicellular pattern on a 2D grid all by using differentiable update rules without any global update clock.\n\nWe ll be writing our training loops,callbacks and hooks using MiniAi. MiniAI is a small and flexible library which goes under the hood, and gives us the flexibility to customize every part of model training - from the model initialization to writing hooks to look inside our model. Let’s go ahead and install MiniAi.\n\n!pip install -Uqq git+https://github.com/fastai/course22p2\n\n\npip install git+https://github.com/huggingface/transformers\n\n\n\n\nimport pickle,gzip,math,os,time,shutil,torch,random,timm,torchvision,io,PIL, einops\nimport fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom IPython.display import display, clear_output, HTML\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\nfrom fastcore.foundation import L, store_attr\nfrom PIL import Image\nimport base64\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.init import *\nfrom miniai.sgd import *\nfrom miniai.resnet import *\n\n\n\n\ndef download_image(url):\n    imgb = fc.urlread(url, decode=False) \n    return torchvision.io.decode_image(tensor(list(imgb), dtype=torch.uint8)).float()/255.\nurl = \"https://images.pexels.com/photos/34225/spider-web-with-water-beads-network-dewdrop.jpg?w=256\"\n# url = \"https://as2.ftcdn.net/v2/jpg/04/67/20/91/1000_F_467209130_vMox1GNkLxsrL4S9v3tWGoMOeNoGSJT2.jpg\"\n# url = \"https://www.robots.ox.ac.uk/~vgg/data/dtd/thumbs/dotted/dotted_0201.jpg\"\n# url = 'https://www.robots.ox.ac.uk/~vgg/data/dtd/thumbs/bubbly/bubbly_0101.jpg'\n# url = \"https://www.freevector.com/uploads/vector/preview/17677/FreeVector-Leaf-Pattern.jpg\"\n\nstyle_im = download_image(url).to(def_device)\nshow_image(style_im);\n\n\n\n\n\n\n\n\n\n\n\nWe will use Vgg16 to extract features from the style image. The resultant feature maps contain low level feature representations of the style image encoded sptially. But we don’t want that. We need encoded information of patterns, for that we need to figure out what is the co-relation between these low-level features. Think about it like this, just having individual feature maps would give you the representation of each feature (a curve, dots, edges etc), but the degree of corelation between these features encoded in a matrix would be able to identify the patterns that appear in the image. We will use Gram Matrices to compute these - for a feature map with f features in an h x w grid, we’ll flatten out the spatial component and then for every feature we’ll take the dot product of that row with itself, giving an f x f matrix as the result.\nOur loss function will be the L2 Loss between the Gram matrix of Style image, and the iteratively updated Gram matrix of the input random noise grid as keep going through the forward and backward passes of our NCA model.\nThe picture below eloquently captures what a Gram Matrix is\n\n\n\nvgg16 = timm.create_model('vgg16', pretrained=True).to(def_device).features\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\ndef calc_features(imgs, target_layers=[18, 25]): \n    x = normalize(imgs)\n    feats = []\n    for i, layer in enumerate(vgg16[:max(target_layers)+1]):\n        x = layer(x)\n        if i in target_layers:\n            feats.append(x.clone())\n    return feats\n\ndef calc_grams(img, target_layers=[1, 6, 11, 18, 25]):\n    return L(torch.einsum('bchw, bdhw -&gt; cd', x, x) / (x.shape[-2]*x.shape[-1])\n            for x in calc_features(img, target_layers))\n\nclass StyleLossToTarget():\n    def __init__(self, target_im, target_layers=[1, 6, 11, 18, 25]):\n        fc.store_attr()\n        with torch.no_grad(): self.target_grams = calc_grams(target_im[None], target_layers)\n    def __call__(self, input_im): \n        return sum((f1-f2).pow(2).mean() for f1, f2 in \n               zip(calc_grams(input_im, self.target_layers), self.target_grams))\n\nstyle_loss = StyleLossToTarget(style_im)\nstyle_loss(torch.rand(1, 3, 256, 256).to(def_device))\n\n\n\n\ntensor(1179.7140, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)\n\n\n\nvgg16\n\nSequential(\n  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU(inplace=True)\n  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (3): ReLU(inplace=True)\n  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (6): ReLU(inplace=True)\n  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (8): ReLU(inplace=True)\n  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (11): ReLU(inplace=True)\n  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (13): ReLU(inplace=True)\n  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (15): ReLU(inplace=True)\n  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (18): ReLU(inplace=True)\n  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (20): ReLU(inplace=True)\n  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (22): ReLU(inplace=True)\n  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (25): ReLU(inplace=True)\n  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (27): ReLU(inplace=True)\n  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (29): ReLU(inplace=True)\n  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\n\n\nLet’s set up our NCA Model now\n\nnum_channels = 4\nhidden_n = 8\n\nWe will have intial grids with the image size which we want and values of each cell will be 0. We make one grid with grid size of 128 and channels as defined above\n\ndef make_grids(n, sz=128): return torch.zeros(n, num_channels, sz, sz).to(def_device)\nx = make_grids(1)\nx.shape\n\ntorch.Size([1, 4, 128, 128])\n\n\nNow we will have our pre defined filters which we will aplly across our grids. These are four 3x3 filters which we will apply acorss each of the 4 channels individually\n\n# Hard-coded filters\nfilters = torch.stack([\n    tensor([[0.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,0.0]]),\n    tensor([[-1.0,0.0,1.0],[-2.0,0.0,2.0],[-1.0,0.0,1.0]]),\n    tensor([[-1.0,0.0,1.0],[-2.0,0.0,2.0],[-1.0,0.0,1.0]]).T,\n    tensor([[1.0,2.0,1.0],[2.0,-12,2.0],[1.0,2.0,1.0]])\n]).to(def_device)\nfilters.shape\n\ntorch.Size([4, 3, 3])\n\n\n\ndef perchannel_conv(x, filters):\n    '''filters: [filter_n, h, w]'''\n    b, ch, h, w = x.shape\n    y = x.reshape(b*ch, 1, h, w)\n    y = F.pad(y, [1, 1, 1, 1], 'circular') # &lt;&lt; Note pad mode\n    y = F.conv2d(y, filters[:,None])\n    return y.reshape(b, -1, h, w)\n\nWe apply these filters across every four channels of our input zero grids, to have the final 16 channels\n\nmodel_inputs = perchannel_conv(x, filters)\nmodel_inputs.shape\n\ntorch.Size([1, 16, 128, 128])\n\n\n\nfilters.shape[0]\n\n4\n\n\n\n\n\n\nWe can now go ahead and define our NCA model class\nFew things to note in the NCA class we have written - We zero out the weights of the second layer while initializing - By initializing the weights to zero, you provide a starting point that might encourage the model to learn meaningful features from the data. - Random update: only update ~50% of the cells. This is just like applying Dropout - This Random update is inspired by biology where in a biological system not all updates are done with respect to any global clock at the same instance while it grows or updates. Similarily we add randomess to all the updates in our model\n\nclass SimpleCA(nn.Module):\n    def __init__(self, zero_w2=True):\n        super().__init__()\n        self.w1 = nn.Conv2d(num_channels*4, hidden_n, 1)\n        self.relu = nn.ReLU()\n        self.w2 = nn.Conv2d(hidden_n, num_channels, 1, bias=False)\n        if zero_w2: self.w2.weight.data.zero_()\n\n\n    def forward(self, x, update_rate=0.5):\n        y = perchannel_conv(x, filters) # Apply the filters\n        y = self.w2(self.relu(self.w1(y))) # pass the result through our simple neural network\n        b, c, h, w = y.shape\n        update_mask = (torch.rand(b, 1, h, w).to(x.device)+update_rate).floor() # Random update\n        return x+y*update_mask\n\n    def to_rgb(self, x):\n        return x[...,:3,:,:]+0.5\n\n\n\n\nWe will be using MiniAi framework and its customised methods(classes,callbacks etc) to write our training loop\n\nclass LengthDataset():\n    def __init__(self, length=1): self.length=length\n    def __len__(self): return self.length\n    def __getitem__(self, idx): return 0,0\n\ndef get_dummy_dls(length=100):\n    return DataLoaders(DataLoader(LengthDataset(length), batch_size=1),\n                       DataLoader(LengthDataset(1), batch_size=1))\n\nWe following is a callback that plots graphs and training metrics using MiniAi\n\nclass NCAProgressCB(ProgressCB):\n    def after_batch(self, learn):\n        learn.dl.comment = f'{learn.loss:.3f}'\n        if not (hasattr(learn, 'metrics') and learn.training): return \n        self.losses.append(learn.loss.item())\n        mbar = self.mbar\n        if not hasattr(mbar, 'graph_fig'):\n            mbar.graph_fig, mbar.graph_axs = plt.subplots(1, 2, figsize=(12, 3.5))\n            mbar.graph_out = display(mbar.graph_fig, display_id=True)\n\n        # Update preview image every 64 iters\n        if (len(self.losses))%64 != 10: return \n        \n        # Plot losses:\n        mbar.graph_axs[0].clear()\n        mbar.graph_axs[0].plot(self.losses, '.', alpha=0.3)\n        mbar.graph_axs[0].set_yscale('log')\n        mbar.graph_axs[0].set_ylim(tensor(self.losses).min(), self.losses[0])\n        \n        # Show preview images:\n        rgb = learn.model.to_rgb(learn.preds.detach()).clip(0, 1)\n        show_image(torchvision.utils.make_grid(rgb), ax=mbar.graph_axs[1])\n        \n        # Update graph\n        mbar.graph_out.update(mbar.graph_fig)\n\nAlong with Style Loss which we defined above, we also use overflow loss which penalises predictions that overflow beyond a range (-1 to 1 in our case)\n\nclass NCACB(TrainCB):\n    order = DeviceCB.order+1\n    def __init__(self, ca, style_img_tensor, style_loss_scale=0.1, size=256, \n                 step_n_min=32, step_n_max=96, batch_size=4):\n        fc.store_attr()\n        with torch.no_grad(): self.pool = make_grids(256, sz=size) # Set up a 'pool' of grids\n    \n    def predict(self, learn): \n        \n        # Pick some random samples from the pool\n        #we select random 4 numbers to be the current batch and use those grid from the pool\n        batch_idx = torch.randint(0, len(self.pool), (self.batch_size,))\n        x = self.pool[batch_idx]\n        \n        # occasionally zero out some samples\n        #if a random number is less than 1 from selecting 1 to 7, it zeros out \n        if torch.randint(8, (1,)) &lt; 1: \n            x[:1] =  make_grids(1, sz=self.size)\n        \n        # Apply the model a number of times\n        for _ in range(torch.randint(self.step_n_min, self.step_n_max, (1,))):\n            x = learn.model(x)\n        \n        # Update pool\n        with torch.no_grad(): self.pool[batch_idx] = x\n        \n        # and store preds\n        learn.preds = x\n        \n    def get_loss(self, learn): \n        style_loss = learn.loss_func(learn.model.to_rgb(self.learn.preds))\n        overflow_loss = (learn.preds-learn.preds.clamp(-1.0, 1.0)).abs().sum()\n        learn.loss = overflow_loss + style_loss*self.style_loss_scale\n        \n    def backward(self, learn):\n        learn.loss.backward()\n        # Gradient normalization:\n        for p in learn.model.parameters():\n            p.grad /= (p.grad.norm()+1e-8) \n        \n    def before_fit(self, learn): self.learn=learn \n\n\nmodel = SimpleCA().to(def_device)\ncbs = [NCACB(model, style_im), NCAProgressCB(), MetricsCB()]\nstyle_loss = StyleLossToTarget(style_im)\n\nlearn = Learner(model, get_dummy_dls(1200), style_loss, lr=1e-3, cbs=cbs, opt_func=torch.optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n164.076\n0\ntrain\n\n\n26.250\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBelow is the final batch of images in training\n\n# Check out the final batch:\nrgb = model.to_rgb(learn.preds.detach())\nrgb = torchvision.utils.make_grid(rgb)\nshow_image(rgb.clip(0, 1));\n\n\n\n\n\n\n\n\nLet’s put our trained NCA model to use. We start of with a random noise grid as usual, and apply the model to this grid for n number of times(900 below). With a few hundred steps we see the patterns forming from our style image. If we look closely we can see how eloquently these patterns are forming, with even spacing and ending properly around edges. All this with a model with just a few hundred paramerers. Having the ability to capture regenerative, complex patterns with just few hundred parameters is the real beauty of these NCA self organising models.\n\n# Apply a numbe of times to a random initial starting grid:\nimages = []\nx = torch.randn(1, num_channels, 128, 128).to(def_device) * 0.1\nwith torch.no_grad():\n    for i in range(1200):\n        x = model(x)\n        if i%100==0: images.append(model.to_rgb(x)[0].clip(0, 1))\nshow_images(images)\n\n\n\n\n\n\n\n\nSo just with 168 weight parameters, we have a model which can create patterns of the style image from random noise\n\nsum(p.numel() for p in model.parameters()) # !!\n\n168\n\n\n\n\n\nLet’s put our images we get from our model at different timesteps, into a video to see the progress of how the patterns develop over time as the grid goes from noise to changing some pixels to finally developing mature patterns present in the Style Image.\n\ndef progress_video():\n  # Turn the images in steps/ into a video with ffmpeg\n  !ffmpeg -y -v 0 -framerate 24 -i steps/%05d.jpeg video.mp4\n\n  # Display it inline\n  mp4 = open('video.mp4','rb').read()\n  data_url = \"data:video/mp4;base64,\" + base64.b64encode(mp4).decode()\n  return HTML(\"\"\"\n  &lt;video width=256 controls&gt;\n        &lt;source src=\"%s\" type=\"video/mp4\"&gt;\n  &lt;/video&gt;\n  \"\"\" % data_url)\n\n\n!mkdir -p steps\n!rm steps/*\nx = torch.randn(1, num_channels, 128, 128).to(def_device) * 0.1\nfor i in range(900):\n  with torch.no_grad():\n    x = model(x)\n    img = model.to_rgb(x).detach().cpu().clip(0, 1).squeeze().permute(1, 2, 0)\n    img = Image.fromarray(np.array(img*255).astype(np.uint8))\n    img.save(f'steps/{i:05}.jpeg')\nprogress_video()\n\n\n  \n        \n  \n  \n\n\n\n\n\nIn the post today, we tried out the vanilla NCA model. We saw the power of self organising systems.We made pretty pictures using differentiable self-organizing systems. There is a lot of scope to make this model detect and regenerate complex persisting patterns. We only saw a small glimpse of self organising systems. We can add more kernels, make the model deeper, try out different loss function among other improvements to see how complex patterns these models can capture, and regenerate. We can erase out part of the pattern from the image and still the model will be able to regenerate the pattern with just the knowledge of the neighbouring cells. We will try this and much more in the upcoming posts."
  },
  {
    "objectID": "posts/nca/index.html#importing-libraries",
    "href": "posts/nca/index.html#importing-libraries",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "import pickle,gzip,math,os,time,shutil,torch,random,timm,torchvision,io,PIL, einops\nimport fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom IPython.display import display, clear_output, HTML\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\nfrom fastcore.foundation import L, store_attr\nfrom PIL import Image\nimport base64\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.init import *\nfrom miniai.sgd import *\nfrom miniai.resnet import *\n\n\n\n\ndef download_image(url):\n    imgb = fc.urlread(url, decode=False) \n    return torchvision.io.decode_image(tensor(list(imgb), dtype=torch.uint8)).float()/255.\nurl = \"https://images.pexels.com/photos/34225/spider-web-with-water-beads-network-dewdrop.jpg?w=256\"\n# url = \"https://as2.ftcdn.net/v2/jpg/04/67/20/91/1000_F_467209130_vMox1GNkLxsrL4S9v3tWGoMOeNoGSJT2.jpg\"\n# url = \"https://www.robots.ox.ac.uk/~vgg/data/dtd/thumbs/dotted/dotted_0201.jpg\"\n# url = 'https://www.robots.ox.ac.uk/~vgg/data/dtd/thumbs/bubbly/bubbly_0101.jpg'\n# url = \"https://www.freevector.com/uploads/vector/preview/17677/FreeVector-Leaf-Pattern.jpg\"\n\nstyle_im = download_image(url).to(def_device)\nshow_image(style_im);\n\n\n\n\n\n\n\n\n\n\n\nWe will use Vgg16 to extract features from the style image. The resultant feature maps contain low level feature representations of the style image encoded sptially. But we don’t want that. We need encoded information of patterns, for that we need to figure out what is the co-relation between these low-level features. Think about it like this, just having individual feature maps would give you the representation of each feature (a curve, dots, edges etc), but the degree of corelation between these features encoded in a matrix would be able to identify the patterns that appear in the image. We will use Gram Matrices to compute these - for a feature map with f features in an h x w grid, we’ll flatten out the spatial component and then for every feature we’ll take the dot product of that row with itself, giving an f x f matrix as the result.\nOur loss function will be the L2 Loss between the Gram matrix of Style image, and the iteratively updated Gram matrix of the input random noise grid as keep going through the forward and backward passes of our NCA model.\nThe picture below eloquently captures what a Gram Matrix is\n\n\n\nvgg16 = timm.create_model('vgg16', pretrained=True).to(def_device).features\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\ndef calc_features(imgs, target_layers=[18, 25]): \n    x = normalize(imgs)\n    feats = []\n    for i, layer in enumerate(vgg16[:max(target_layers)+1]):\n        x = layer(x)\n        if i in target_layers:\n            feats.append(x.clone())\n    return feats\n\ndef calc_grams(img, target_layers=[1, 6, 11, 18, 25]):\n    return L(torch.einsum('bchw, bdhw -&gt; cd', x, x) / (x.shape[-2]*x.shape[-1])\n            for x in calc_features(img, target_layers))\n\nclass StyleLossToTarget():\n    def __init__(self, target_im, target_layers=[1, 6, 11, 18, 25]):\n        fc.store_attr()\n        with torch.no_grad(): self.target_grams = calc_grams(target_im[None], target_layers)\n    def __call__(self, input_im): \n        return sum((f1-f2).pow(2).mean() for f1, f2 in \n               zip(calc_grams(input_im, self.target_layers), self.target_grams))\n\nstyle_loss = StyleLossToTarget(style_im)\nstyle_loss(torch.rand(1, 3, 256, 256).to(def_device))\n\n\n\n\ntensor(1179.7140, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)\n\n\n\nvgg16\n\nSequential(\n  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU(inplace=True)\n  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (3): ReLU(inplace=True)\n  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (6): ReLU(inplace=True)\n  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (8): ReLU(inplace=True)\n  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (11): ReLU(inplace=True)\n  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (13): ReLU(inplace=True)\n  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (15): ReLU(inplace=True)\n  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (18): ReLU(inplace=True)\n  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (20): ReLU(inplace=True)\n  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (22): ReLU(inplace=True)\n  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (25): ReLU(inplace=True)\n  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (27): ReLU(inplace=True)\n  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (29): ReLU(inplace=True)\n  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\n\n\nLet’s set up our NCA Model now\n\nnum_channels = 4\nhidden_n = 8\n\nWe will have intial grids with the image size which we want and values of each cell will be 0. We make one grid with grid size of 128 and channels as defined above\n\ndef make_grids(n, sz=128): return torch.zeros(n, num_channels, sz, sz).to(def_device)\nx = make_grids(1)\nx.shape\n\ntorch.Size([1, 4, 128, 128])\n\n\nNow we will have our pre defined filters which we will aplly across our grids. These are four 3x3 filters which we will apply acorss each of the 4 channels individually\n\n# Hard-coded filters\nfilters = torch.stack([\n    tensor([[0.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,0.0]]),\n    tensor([[-1.0,0.0,1.0],[-2.0,0.0,2.0],[-1.0,0.0,1.0]]),\n    tensor([[-1.0,0.0,1.0],[-2.0,0.0,2.0],[-1.0,0.0,1.0]]).T,\n    tensor([[1.0,2.0,1.0],[2.0,-12,2.0],[1.0,2.0,1.0]])\n]).to(def_device)\nfilters.shape\n\ntorch.Size([4, 3, 3])\n\n\n\ndef perchannel_conv(x, filters):\n    '''filters: [filter_n, h, w]'''\n    b, ch, h, w = x.shape\n    y = x.reshape(b*ch, 1, h, w)\n    y = F.pad(y, [1, 1, 1, 1], 'circular') # &lt;&lt; Note pad mode\n    y = F.conv2d(y, filters[:,None])\n    return y.reshape(b, -1, h, w)\n\nWe apply these filters across every four channels of our input zero grids, to have the final 16 channels\n\nmodel_inputs = perchannel_conv(x, filters)\nmodel_inputs.shape\n\ntorch.Size([1, 16, 128, 128])\n\n\n\nfilters.shape[0]\n\n4"
  },
  {
    "objectID": "posts/nca/index.html#nca-model",
    "href": "posts/nca/index.html#nca-model",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "We can now go ahead and define our NCA model class\nFew things to note in the NCA class we have written - We zero out the weights of the second layer while initializing - By initializing the weights to zero, you provide a starting point that might encourage the model to learn meaningful features from the data. - Random update: only update ~50% of the cells. This is just like applying Dropout - This Random update is inspired by biology where in a biological system not all updates are done with respect to any global clock at the same instance while it grows or updates. Similarily we add randomess to all the updates in our model\n\nclass SimpleCA(nn.Module):\n    def __init__(self, zero_w2=True):\n        super().__init__()\n        self.w1 = nn.Conv2d(num_channels*4, hidden_n, 1)\n        self.relu = nn.ReLU()\n        self.w2 = nn.Conv2d(hidden_n, num_channels, 1, bias=False)\n        if zero_w2: self.w2.weight.data.zero_()\n\n\n    def forward(self, x, update_rate=0.5):\n        y = perchannel_conv(x, filters) # Apply the filters\n        y = self.w2(self.relu(self.w1(y))) # pass the result through our simple neural network\n        b, c, h, w = y.shape\n        update_mask = (torch.rand(b, 1, h, w).to(x.device)+update_rate).floor() # Random update\n        return x+y*update_mask\n\n    def to_rgb(self, x):\n        return x[...,:3,:,:]+0.5"
  },
  {
    "objectID": "posts/nca/index.html#lets-get-training",
    "href": "posts/nca/index.html#lets-get-training",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "We will be using MiniAi framework and its customised methods(classes,callbacks etc) to write our training loop\n\nclass LengthDataset():\n    def __init__(self, length=1): self.length=length\n    def __len__(self): return self.length\n    def __getitem__(self, idx): return 0,0\n\ndef get_dummy_dls(length=100):\n    return DataLoaders(DataLoader(LengthDataset(length), batch_size=1),\n                       DataLoader(LengthDataset(1), batch_size=1))\n\nWe following is a callback that plots graphs and training metrics using MiniAi\n\nclass NCAProgressCB(ProgressCB):\n    def after_batch(self, learn):\n        learn.dl.comment = f'{learn.loss:.3f}'\n        if not (hasattr(learn, 'metrics') and learn.training): return \n        self.losses.append(learn.loss.item())\n        mbar = self.mbar\n        if not hasattr(mbar, 'graph_fig'):\n            mbar.graph_fig, mbar.graph_axs = plt.subplots(1, 2, figsize=(12, 3.5))\n            mbar.graph_out = display(mbar.graph_fig, display_id=True)\n\n        # Update preview image every 64 iters\n        if (len(self.losses))%64 != 10: return \n        \n        # Plot losses:\n        mbar.graph_axs[0].clear()\n        mbar.graph_axs[0].plot(self.losses, '.', alpha=0.3)\n        mbar.graph_axs[0].set_yscale('log')\n        mbar.graph_axs[0].set_ylim(tensor(self.losses).min(), self.losses[0])\n        \n        # Show preview images:\n        rgb = learn.model.to_rgb(learn.preds.detach()).clip(0, 1)\n        show_image(torchvision.utils.make_grid(rgb), ax=mbar.graph_axs[1])\n        \n        # Update graph\n        mbar.graph_out.update(mbar.graph_fig)\n\nAlong with Style Loss which we defined above, we also use overflow loss which penalises predictions that overflow beyond a range (-1 to 1 in our case)\n\nclass NCACB(TrainCB):\n    order = DeviceCB.order+1\n    def __init__(self, ca, style_img_tensor, style_loss_scale=0.1, size=256, \n                 step_n_min=32, step_n_max=96, batch_size=4):\n        fc.store_attr()\n        with torch.no_grad(): self.pool = make_grids(256, sz=size) # Set up a 'pool' of grids\n    \n    def predict(self, learn): \n        \n        # Pick some random samples from the pool\n        #we select random 4 numbers to be the current batch and use those grid from the pool\n        batch_idx = torch.randint(0, len(self.pool), (self.batch_size,))\n        x = self.pool[batch_idx]\n        \n        # occasionally zero out some samples\n        #if a random number is less than 1 from selecting 1 to 7, it zeros out \n        if torch.randint(8, (1,)) &lt; 1: \n            x[:1] =  make_grids(1, sz=self.size)\n        \n        # Apply the model a number of times\n        for _ in range(torch.randint(self.step_n_min, self.step_n_max, (1,))):\n            x = learn.model(x)\n        \n        # Update pool\n        with torch.no_grad(): self.pool[batch_idx] = x\n        \n        # and store preds\n        learn.preds = x\n        \n    def get_loss(self, learn): \n        style_loss = learn.loss_func(learn.model.to_rgb(self.learn.preds))\n        overflow_loss = (learn.preds-learn.preds.clamp(-1.0, 1.0)).abs().sum()\n        learn.loss = overflow_loss + style_loss*self.style_loss_scale\n        \n    def backward(self, learn):\n        learn.loss.backward()\n        # Gradient normalization:\n        for p in learn.model.parameters():\n            p.grad /= (p.grad.norm()+1e-8) \n        \n    def before_fit(self, learn): self.learn=learn \n\n\nmodel = SimpleCA().to(def_device)\ncbs = [NCACB(model, style_im), NCAProgressCB(), MetricsCB()]\nstyle_loss = StyleLossToTarget(style_im)\n\nlearn = Learner(model, get_dummy_dls(1200), style_loss, lr=1e-3, cbs=cbs, opt_func=torch.optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n164.076\n0\ntrain\n\n\n26.250\n0\neval"
  },
  {
    "objectID": "posts/nca/index.html#results",
    "href": "posts/nca/index.html#results",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "Below is the final batch of images in training\n\n# Check out the final batch:\nrgb = model.to_rgb(learn.preds.detach())\nrgb = torchvision.utils.make_grid(rgb)\nshow_image(rgb.clip(0, 1));\n\n\n\n\n\n\n\n\nLet’s put our trained NCA model to use. We start of with a random noise grid as usual, and apply the model to this grid for n number of times(900 below). With a few hundred steps we see the patterns forming from our style image. If we look closely we can see how eloquently these patterns are forming, with even spacing and ending properly around edges. All this with a model with just a few hundred paramerers. Having the ability to capture regenerative, complex patterns with just few hundred parameters is the real beauty of these NCA self organising models.\n\n# Apply a numbe of times to a random initial starting grid:\nimages = []\nx = torch.randn(1, num_channels, 128, 128).to(def_device) * 0.1\nwith torch.no_grad():\n    for i in range(1200):\n        x = model(x)\n        if i%100==0: images.append(model.to_rgb(x)[0].clip(0, 1))\nshow_images(images)\n\n\n\n\n\n\n\n\nSo just with 168 weight parameters, we have a model which can create patterns of the style image from random noise\n\nsum(p.numel() for p in model.parameters()) # !!\n\n168"
  },
  {
    "objectID": "posts/nca/index.html#video",
    "href": "posts/nca/index.html#video",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "Let’s put our images we get from our model at different timesteps, into a video to see the progress of how the patterns develop over time as the grid goes from noise to changing some pixels to finally developing mature patterns present in the Style Image.\n\ndef progress_video():\n  # Turn the images in steps/ into a video with ffmpeg\n  !ffmpeg -y -v 0 -framerate 24 -i steps/%05d.jpeg video.mp4\n\n  # Display it inline\n  mp4 = open('video.mp4','rb').read()\n  data_url = \"data:video/mp4;base64,\" + base64.b64encode(mp4).decode()\n  return HTML(\"\"\"\n  &lt;video width=256 controls&gt;\n        &lt;source src=\"%s\" type=\"video/mp4\"&gt;\n  &lt;/video&gt;\n  \"\"\" % data_url)\n\n\n!mkdir -p steps\n!rm steps/*\nx = torch.randn(1, num_channels, 128, 128).to(def_device) * 0.1\nfor i in range(900):\n  with torch.no_grad():\n    x = model(x)\n    img = model.to_rgb(x).detach().cpu().clip(0, 1).squeeze().permute(1, 2, 0)\n    img = Image.fromarray(np.array(img*255).astype(np.uint8))\n    img.save(f'steps/{i:05}.jpeg')\nprogress_video()"
  },
  {
    "objectID": "posts/nca/index.html#conclusion",
    "href": "posts/nca/index.html#conclusion",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "In the post today, we tried out the vanilla NCA model. We saw the power of self organising systems.We made pretty pictures using differentiable self-organizing systems. There is a lot of scope to make this model detect and regenerate complex persisting patterns. We only saw a small glimpse of self organising systems. We can add more kernels, make the model deeper, try out different loss function among other improvements to see how complex patterns these models can capture, and regenerate. We can erase out part of the pattern from the image and still the model will be able to regenerate the pattern with just the knowledge of the neighbouring cells. We will try this and much more in the upcoming posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Sehaj, and this blog is my personal exploration into the depths of artificial intelligence and machine learning. Here, I share my insights, reflections, and opinions on the latest technologies, models, and papers that capture my curiosity."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sehaj-notepad",
    "section": "",
    "text": "Neural Cellular Automata\n\n\n\n\n\n\nAI\n\n\npython\n\n\njupyter\n\n\ncode\n\n\n\nSelf-organizing systems of simple cells can produce complex patterns.\n\n\n\n\n\nFeb 5, 2024\n\n\nSehajdeep Singh\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 2, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  }
]