[
  {
    "objectID": "posts/nca/index.html",
    "href": "posts/nca/index.html",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "What we are going to do is to try to create a diffrentiable self-organising system Neural Cellular Automata with just a few hundred parameters.We set up a system of ‘cells’, often represented by pixels in a line or a grid. Each cell can ‘see’ their immediate neighbors, and can change it’s output based on this information. We ll use Cellular Automata models to identifying cell-level rules which give rise to complex, regenerative behavior of the collective.\nCellular Automatas consist of a grid of cells being iteratively updated, with the same set of rules being applied to each cell at every step.The new state of a cell depends only on the states of the few cells in its immediate neighborhood.\nSo to put it in simple words, we start out with a random grid with pure noise pixels. Each cell in the grid only knows about the states of its immediate neighbourhood cells.With this structure in place, we will produce a predefined multicellular pattern on a 2D grid all by using differentiable update rules without any global update clock.\n\nWe ll be writing our training loops,callbacks and hooks using MiniAi. MiniAI is a small and flexible library which goes under the hood, and gives us the flexibility to customize every part of model training - from the model initialization to writing hooks to look inside our model. Let’s go ahead and install MiniAi.\n\n!pip install -Uqq git+https://github.com/fastai/course22p2\n\n\npip install git+https://github.com/huggingface/transformers\n\n\n\n\nimport pickle,gzip,math,os,time,shutil,torch,random,timm,torchvision,io,PIL, einops\nimport fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom IPython.display import display, clear_output, HTML\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\nfrom fastcore.foundation import L, store_attr\nfrom PIL import Image\nimport base64\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.init import *\nfrom miniai.sgd import *\nfrom miniai.resnet import *\n\n\n\n\ndef download_image(url):\n    imgb = fc.urlread(url, decode=False) \n    return torchvision.io.decode_image(tensor(list(imgb), dtype=torch.uint8)).float()/255.\nurl = \"https://images.pexels.com/photos/34225/spider-web-with-water-beads-network-dewdrop.jpg?w=256\"\n# url = \"https://as2.ftcdn.net/v2/jpg/04/67/20/91/1000_F_467209130_vMox1GNkLxsrL4S9v3tWGoMOeNoGSJT2.jpg\"\n# url = \"https://www.robots.ox.ac.uk/~vgg/data/dtd/thumbs/dotted/dotted_0201.jpg\"\n# url = 'https://www.robots.ox.ac.uk/~vgg/data/dtd/thumbs/bubbly/bubbly_0101.jpg'\n# url = \"https://www.freevector.com/uploads/vector/preview/17677/FreeVector-Leaf-Pattern.jpg\"\n\nstyle_im = download_image(url).to(def_device)\nshow_image(style_im);\n\n\n\n\n\n\n\n\n\n\n\nWe will use Vgg16 to extract features from the style image. The resultant feature maps contain low level feature representations of the style image encoded sptially. But we don’t want that. We need encoded information of patterns, for that we need to figure out what is the co-relation between these low-level features. Think about it like this, just having individual feature maps would give you the representation of each feature (a curve, dots, edges etc), but the degree of corelation between these features encoded in a matrix would be able to identify the patterns that appear in the image. We will use Gram Matrices to compute these - for a feature map with f features in an h x w grid, we’ll flatten out the spatial component and then for every feature we’ll take the dot product of that row with itself, giving an f x f matrix as the result.\nOur loss function will be the L2 Loss between the Gram matrix of Style image, and the iteratively updated Gram matrix of the input random noise grid as keep going through the forward and backward passes of our NCA model.\nThe picture below eloquently captures what a Gram Matrix is\n\n\n\nvgg16 = timm.create_model('vgg16', pretrained=True).to(def_device).features\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\ndef calc_features(imgs, target_layers=[18, 25]): \n    x = normalize(imgs)\n    feats = []\n    for i, layer in enumerate(vgg16[:max(target_layers)+1]):\n        x = layer(x)\n        if i in target_layers:\n            feats.append(x.clone())\n    return feats\n\ndef calc_grams(img, target_layers=[1, 6, 11, 18, 25]):\n    return L(torch.einsum('bchw, bdhw -&gt; cd', x, x) / (x.shape[-2]*x.shape[-1])\n            for x in calc_features(img, target_layers))\n\nclass StyleLossToTarget():\n    def __init__(self, target_im, target_layers=[1, 6, 11, 18, 25]):\n        fc.store_attr()\n        with torch.no_grad(): self.target_grams = calc_grams(target_im[None], target_layers)\n    def __call__(self, input_im): \n        return sum((f1-f2).pow(2).mean() for f1, f2 in \n               zip(calc_grams(input_im, self.target_layers), self.target_grams))\n\nstyle_loss = StyleLossToTarget(style_im)\nstyle_loss(torch.rand(1, 3, 256, 256).to(def_device))\n\n\n\n\ntensor(1179.7140, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)\n\n\n\nvgg16\n\nSequential(\n  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU(inplace=True)\n  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (3): ReLU(inplace=True)\n  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (6): ReLU(inplace=True)\n  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (8): ReLU(inplace=True)\n  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (11): ReLU(inplace=True)\n  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (13): ReLU(inplace=True)\n  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (15): ReLU(inplace=True)\n  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (18): ReLU(inplace=True)\n  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (20): ReLU(inplace=True)\n  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (22): ReLU(inplace=True)\n  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (25): ReLU(inplace=True)\n  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (27): ReLU(inplace=True)\n  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (29): ReLU(inplace=True)\n  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\n\n\nLet’s set up our NCA Model now\n\nnum_channels = 4\nhidden_n = 8\n\nWe will have intial grids with the image size which we want and values of each cell will be 0. We make one grid with grid size of 128 and channels as defined above\n\ndef make_grids(n, sz=128): return torch.zeros(n, num_channels, sz, sz).to(def_device)\nx = make_grids(1)\nx.shape\n\ntorch.Size([1, 4, 128, 128])\n\n\nNow we will have our pre defined filters which we will aplly across our grids. These are four 3x3 filters which we will apply acorss each of the 4 channels individually\n\n# Hard-coded filters\nfilters = torch.stack([\n    tensor([[0.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,0.0]]),\n    tensor([[-1.0,0.0,1.0],[-2.0,0.0,2.0],[-1.0,0.0,1.0]]),\n    tensor([[-1.0,0.0,1.0],[-2.0,0.0,2.0],[-1.0,0.0,1.0]]).T,\n    tensor([[1.0,2.0,1.0],[2.0,-12,2.0],[1.0,2.0,1.0]])\n]).to(def_device)\nfilters.shape\n\ntorch.Size([4, 3, 3])\n\n\n\ndef perchannel_conv(x, filters):\n    '''filters: [filter_n, h, w]'''\n    b, ch, h, w = x.shape\n    y = x.reshape(b*ch, 1, h, w)\n    y = F.pad(y, [1, 1, 1, 1], 'circular') # &lt;&lt; Note pad mode\n    y = F.conv2d(y, filters[:,None])\n    return y.reshape(b, -1, h, w)\n\nWe apply these filters across every four channels of our input zero grids, to have the final 16 channels\n\nmodel_inputs = perchannel_conv(x, filters)\nmodel_inputs.shape\n\ntorch.Size([1, 16, 128, 128])\n\n\n\nfilters.shape[0]\n\n4\n\n\n\n\n\n\nWe can now go ahead and define our NCA model class\nFew things to note in the NCA class we have written - We zero out the weights of the second layer while initializing - By initializing the weights to zero, you provide a starting point that might encourage the model to learn meaningful features from the data. - Random update: only update ~50% of the cells. This is just like applying Dropout - This Random update is inspired by biology where in a biological system not all updates are done with respect to any global clock at the same instance while it grows or updates. Similarily we add randomess to all the updates in our model\n\nclass SimpleCA(nn.Module):\n    def __init__(self, zero_w2=True):\n        super().__init__()\n        self.w1 = nn.Conv2d(num_channels*4, hidden_n, 1)\n        self.relu = nn.ReLU()\n        self.w2 = nn.Conv2d(hidden_n, num_channels, 1, bias=False)\n        if zero_w2: self.w2.weight.data.zero_()\n\n\n    def forward(self, x, update_rate=0.5):\n        y = perchannel_conv(x, filters) # Apply the filters\n        y = self.w2(self.relu(self.w1(y))) # pass the result through our simple neural network\n        b, c, h, w = y.shape\n        update_mask = (torch.rand(b, 1, h, w).to(x.device)+update_rate).floor() # Random update\n        return x+y*update_mask\n\n    def to_rgb(self, x):\n        return x[...,:3,:,:]+0.5\n\n\n\n\nWe will be using MiniAi framework and its customised methods(classes,callbacks etc) to write our training loop\n\nclass LengthDataset():\n    def __init__(self, length=1): self.length=length\n    def __len__(self): return self.length\n    def __getitem__(self, idx): return 0,0\n\ndef get_dummy_dls(length=100):\n    return DataLoaders(DataLoader(LengthDataset(length), batch_size=1),\n                       DataLoader(LengthDataset(1), batch_size=1))\n\nWe following is a callback that plots graphs and training metrics using MiniAi\n\nclass NCAProgressCB(ProgressCB):\n    def after_batch(self, learn):\n        learn.dl.comment = f'{learn.loss:.3f}'\n        if not (hasattr(learn, 'metrics') and learn.training): return \n        self.losses.append(learn.loss.item())\n        mbar = self.mbar\n        if not hasattr(mbar, 'graph_fig'):\n            mbar.graph_fig, mbar.graph_axs = plt.subplots(1, 2, figsize=(12, 3.5))\n            mbar.graph_out = display(mbar.graph_fig, display_id=True)\n\n        # Update preview image every 64 iters\n        if (len(self.losses))%64 != 10: return \n        \n        # Plot losses:\n        mbar.graph_axs[0].clear()\n        mbar.graph_axs[0].plot(self.losses, '.', alpha=0.3)\n        mbar.graph_axs[0].set_yscale('log')\n        mbar.graph_axs[0].set_ylim(tensor(self.losses).min(), self.losses[0])\n        \n        # Show preview images:\n        rgb = learn.model.to_rgb(learn.preds.detach()).clip(0, 1)\n        show_image(torchvision.utils.make_grid(rgb), ax=mbar.graph_axs[1])\n        \n        # Update graph\n        mbar.graph_out.update(mbar.graph_fig)\n\nAlong with Style Loss which we defined above, we also use overflow loss which penalises predictions that overflow beyond a range (-1 to 1 in our case)\n\nclass NCACB(TrainCB):\n    order = DeviceCB.order+1\n    def __init__(self, ca, style_img_tensor, style_loss_scale=0.1, size=256, \n                 step_n_min=32, step_n_max=96, batch_size=4):\n        fc.store_attr()\n        with torch.no_grad(): self.pool = make_grids(256, sz=size) # Set up a 'pool' of grids\n    \n    def predict(self, learn): \n        \n        # Pick some random samples from the pool\n        #we select random 4 numbers to be the current batch and use those grid from the pool\n        batch_idx = torch.randint(0, len(self.pool), (self.batch_size,))\n        x = self.pool[batch_idx]\n        \n        # occasionally zero out some samples\n        #if a random number is less than 1 from selecting 1 to 7, it zeros out \n        if torch.randint(8, (1,)) &lt; 1: \n            x[:1] =  make_grids(1, sz=self.size)\n        \n        # Apply the model a number of times\n        for _ in range(torch.randint(self.step_n_min, self.step_n_max, (1,))):\n            x = learn.model(x)\n        \n        # Update pool\n        with torch.no_grad(): self.pool[batch_idx] = x\n        \n        # and store preds\n        learn.preds = x\n        \n    def get_loss(self, learn): \n        style_loss = learn.loss_func(learn.model.to_rgb(self.learn.preds))\n        overflow_loss = (learn.preds-learn.preds.clamp(-1.0, 1.0)).abs().sum()\n        learn.loss = overflow_loss + style_loss*self.style_loss_scale\n        \n    def backward(self, learn):\n        learn.loss.backward()\n        # Gradient normalization:\n        for p in learn.model.parameters():\n            p.grad /= (p.grad.norm()+1e-8) \n        \n    def before_fit(self, learn): self.learn=learn \n\n\nmodel = SimpleCA().to(def_device)\ncbs = [NCACB(model, style_im), NCAProgressCB(), MetricsCB()]\nstyle_loss = StyleLossToTarget(style_im)\n\nlearn = Learner(model, get_dummy_dls(1200), style_loss, lr=1e-3, cbs=cbs, opt_func=torch.optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n164.076\n0\ntrain\n\n\n26.250\n0\neval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBelow is the final batch of images in training\n\n# Check out the final batch:\nrgb = model.to_rgb(learn.preds.detach())\nrgb = torchvision.utils.make_grid(rgb)\nshow_image(rgb.clip(0, 1));\n\n\n\n\n\n\n\n\nLet’s put our trained NCA model to use. We start of with a random noise grid as usual, and apply the model to this grid for n number of times(900 below). With a few hundred steps we see the patterns forming from our style image. If we look closely we can see how eloquently these patterns are forming, with even spacing and ending properly around edges. All this with a model with just a few hundred paramerers. Having the ability to capture regenerative, complex patterns with just few hundred parameters is the real beauty of these NCA self organising models.\n\n# Apply a numbe of times to a random initial starting grid:\nimages = []\nx = torch.randn(1, num_channels, 128, 128).to(def_device) * 0.1\nwith torch.no_grad():\n    for i in range(1200):\n        x = model(x)\n        if i%100==0: images.append(model.to_rgb(x)[0].clip(0, 1))\nshow_images(images)\n\n\n\n\n\n\n\n\nSo just with 168 weight parameters, we have a model which can create patterns of the style image from random noise\n\nsum(p.numel() for p in model.parameters()) # !!\n\n168\n\n\n\n\n\nLet’s put our images we get from our model at different timesteps, into a video to see the progress of how the patterns develop over time as the grid goes from noise to changing some pixels to finally developing mature patterns present in the Style Image.\n\ndef progress_video():\n  # Turn the images in steps/ into a video with ffmpeg\n  !ffmpeg -y -v 0 -framerate 24 -i steps/%05d.jpeg video.mp4\n\n  # Display it inline\n  mp4 = open('video.mp4','rb').read()\n  data_url = \"data:video/mp4;base64,\" + base64.b64encode(mp4).decode()\n  return HTML(\"\"\"\n  &lt;video width=256 controls&gt;\n        &lt;source src=\"%s\" type=\"video/mp4\"&gt;\n  &lt;/video&gt;\n  \"\"\" % data_url)\n\n\n!mkdir -p steps\n!rm steps/*\nx = torch.randn(1, num_channels, 128, 128).to(def_device) * 0.1\nfor i in range(900):\n  with torch.no_grad():\n    x = model(x)\n    img = model.to_rgb(x).detach().cpu().clip(0, 1).squeeze().permute(1, 2, 0)\n    img = Image.fromarray(np.array(img*255).astype(np.uint8))\n    img.save(f'steps/{i:05}.jpeg')\nprogress_video()\n\n\n  \n        \n  \n  \n\n\n\n\n\nIn the post today, we tried out the vanilla NCA model. We saw the power of self organising systems.We made pretty pictures using differentiable self-organizing systems. There is a lot of scope to make this model detect and regenerate complex persisting patterns. We only saw a small glimpse of self organising systems. We can add more kernels, make the model deeper, try out different loss function among other improvements to see how complex patterns these models can capture, and regenerate. We can erase out part of the pattern from the image and still the model will be able to regenerate the pattern with just the knowledge of the neighbouring cells. We will try this and much more in the upcoming posts."
  },
  {
    "objectID": "posts/nca/index.html#importing-libraries",
    "href": "posts/nca/index.html#importing-libraries",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "import pickle,gzip,math,os,time,shutil,torch,random,timm,torchvision,io,PIL, einops\nimport fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom IPython.display import display, clear_output, HTML\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\nfrom fastcore.foundation import L, store_attr\nfrom PIL import Image\nimport base64\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.init import *\nfrom miniai.sgd import *\nfrom miniai.resnet import *\n\n\n\n\ndef download_image(url):\n    imgb = fc.urlread(url, decode=False) \n    return torchvision.io.decode_image(tensor(list(imgb), dtype=torch.uint8)).float()/255.\nurl = \"https://images.pexels.com/photos/34225/spider-web-with-water-beads-network-dewdrop.jpg?w=256\"\n# url = \"https://as2.ftcdn.net/v2/jpg/04/67/20/91/1000_F_467209130_vMox1GNkLxsrL4S9v3tWGoMOeNoGSJT2.jpg\"\n# url = \"https://www.robots.ox.ac.uk/~vgg/data/dtd/thumbs/dotted/dotted_0201.jpg\"\n# url = 'https://www.robots.ox.ac.uk/~vgg/data/dtd/thumbs/bubbly/bubbly_0101.jpg'\n# url = \"https://www.freevector.com/uploads/vector/preview/17677/FreeVector-Leaf-Pattern.jpg\"\n\nstyle_im = download_image(url).to(def_device)\nshow_image(style_im);\n\n\n\n\n\n\n\n\n\n\n\nWe will use Vgg16 to extract features from the style image. The resultant feature maps contain low level feature representations of the style image encoded sptially. But we don’t want that. We need encoded information of patterns, for that we need to figure out what is the co-relation between these low-level features. Think about it like this, just having individual feature maps would give you the representation of each feature (a curve, dots, edges etc), but the degree of corelation between these features encoded in a matrix would be able to identify the patterns that appear in the image. We will use Gram Matrices to compute these - for a feature map with f features in an h x w grid, we’ll flatten out the spatial component and then for every feature we’ll take the dot product of that row with itself, giving an f x f matrix as the result.\nOur loss function will be the L2 Loss between the Gram matrix of Style image, and the iteratively updated Gram matrix of the input random noise grid as keep going through the forward and backward passes of our NCA model.\nThe picture below eloquently captures what a Gram Matrix is\n\n\n\nvgg16 = timm.create_model('vgg16', pretrained=True).to(def_device).features\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\ndef calc_features(imgs, target_layers=[18, 25]): \n    x = normalize(imgs)\n    feats = []\n    for i, layer in enumerate(vgg16[:max(target_layers)+1]):\n        x = layer(x)\n        if i in target_layers:\n            feats.append(x.clone())\n    return feats\n\ndef calc_grams(img, target_layers=[1, 6, 11, 18, 25]):\n    return L(torch.einsum('bchw, bdhw -&gt; cd', x, x) / (x.shape[-2]*x.shape[-1])\n            for x in calc_features(img, target_layers))\n\nclass StyleLossToTarget():\n    def __init__(self, target_im, target_layers=[1, 6, 11, 18, 25]):\n        fc.store_attr()\n        with torch.no_grad(): self.target_grams = calc_grams(target_im[None], target_layers)\n    def __call__(self, input_im): \n        return sum((f1-f2).pow(2).mean() for f1, f2 in \n               zip(calc_grams(input_im, self.target_layers), self.target_grams))\n\nstyle_loss = StyleLossToTarget(style_im)\nstyle_loss(torch.rand(1, 3, 256, 256).to(def_device))\n\n\n\n\ntensor(1179.7140, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)\n\n\n\nvgg16\n\nSequential(\n  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU(inplace=True)\n  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (3): ReLU(inplace=True)\n  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (6): ReLU(inplace=True)\n  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (8): ReLU(inplace=True)\n  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (11): ReLU(inplace=True)\n  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (13): ReLU(inplace=True)\n  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (15): ReLU(inplace=True)\n  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (18): ReLU(inplace=True)\n  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (20): ReLU(inplace=True)\n  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (22): ReLU(inplace=True)\n  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (25): ReLU(inplace=True)\n  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (27): ReLU(inplace=True)\n  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (29): ReLU(inplace=True)\n  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\n\n\nLet’s set up our NCA Model now\n\nnum_channels = 4\nhidden_n = 8\n\nWe will have intial grids with the image size which we want and values of each cell will be 0. We make one grid with grid size of 128 and channels as defined above\n\ndef make_grids(n, sz=128): return torch.zeros(n, num_channels, sz, sz).to(def_device)\nx = make_grids(1)\nx.shape\n\ntorch.Size([1, 4, 128, 128])\n\n\nNow we will have our pre defined filters which we will aplly across our grids. These are four 3x3 filters which we will apply acorss each of the 4 channels individually\n\n# Hard-coded filters\nfilters = torch.stack([\n    tensor([[0.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,0.0]]),\n    tensor([[-1.0,0.0,1.0],[-2.0,0.0,2.0],[-1.0,0.0,1.0]]),\n    tensor([[-1.0,0.0,1.0],[-2.0,0.0,2.0],[-1.0,0.0,1.0]]).T,\n    tensor([[1.0,2.0,1.0],[2.0,-12,2.0],[1.0,2.0,1.0]])\n]).to(def_device)\nfilters.shape\n\ntorch.Size([4, 3, 3])\n\n\n\ndef perchannel_conv(x, filters):\n    '''filters: [filter_n, h, w]'''\n    b, ch, h, w = x.shape\n    y = x.reshape(b*ch, 1, h, w)\n    y = F.pad(y, [1, 1, 1, 1], 'circular') # &lt;&lt; Note pad mode\n    y = F.conv2d(y, filters[:,None])\n    return y.reshape(b, -1, h, w)\n\nWe apply these filters across every four channels of our input zero grids, to have the final 16 channels\n\nmodel_inputs = perchannel_conv(x, filters)\nmodel_inputs.shape\n\ntorch.Size([1, 16, 128, 128])\n\n\n\nfilters.shape[0]\n\n4"
  },
  {
    "objectID": "posts/nca/index.html#nca-model",
    "href": "posts/nca/index.html#nca-model",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "We can now go ahead and define our NCA model class\nFew things to note in the NCA class we have written - We zero out the weights of the second layer while initializing - By initializing the weights to zero, you provide a starting point that might encourage the model to learn meaningful features from the data. - Random update: only update ~50% of the cells. This is just like applying Dropout - This Random update is inspired by biology where in a biological system not all updates are done with respect to any global clock at the same instance while it grows or updates. Similarily we add randomess to all the updates in our model\n\nclass SimpleCA(nn.Module):\n    def __init__(self, zero_w2=True):\n        super().__init__()\n        self.w1 = nn.Conv2d(num_channels*4, hidden_n, 1)\n        self.relu = nn.ReLU()\n        self.w2 = nn.Conv2d(hidden_n, num_channels, 1, bias=False)\n        if zero_w2: self.w2.weight.data.zero_()\n\n\n    def forward(self, x, update_rate=0.5):\n        y = perchannel_conv(x, filters) # Apply the filters\n        y = self.w2(self.relu(self.w1(y))) # pass the result through our simple neural network\n        b, c, h, w = y.shape\n        update_mask = (torch.rand(b, 1, h, w).to(x.device)+update_rate).floor() # Random update\n        return x+y*update_mask\n\n    def to_rgb(self, x):\n        return x[...,:3,:,:]+0.5"
  },
  {
    "objectID": "posts/nca/index.html#lets-get-training",
    "href": "posts/nca/index.html#lets-get-training",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "We will be using MiniAi framework and its customised methods(classes,callbacks etc) to write our training loop\n\nclass LengthDataset():\n    def __init__(self, length=1): self.length=length\n    def __len__(self): return self.length\n    def __getitem__(self, idx): return 0,0\n\ndef get_dummy_dls(length=100):\n    return DataLoaders(DataLoader(LengthDataset(length), batch_size=1),\n                       DataLoader(LengthDataset(1), batch_size=1))\n\nWe following is a callback that plots graphs and training metrics using MiniAi\n\nclass NCAProgressCB(ProgressCB):\n    def after_batch(self, learn):\n        learn.dl.comment = f'{learn.loss:.3f}'\n        if not (hasattr(learn, 'metrics') and learn.training): return \n        self.losses.append(learn.loss.item())\n        mbar = self.mbar\n        if not hasattr(mbar, 'graph_fig'):\n            mbar.graph_fig, mbar.graph_axs = plt.subplots(1, 2, figsize=(12, 3.5))\n            mbar.graph_out = display(mbar.graph_fig, display_id=True)\n\n        # Update preview image every 64 iters\n        if (len(self.losses))%64 != 10: return \n        \n        # Plot losses:\n        mbar.graph_axs[0].clear()\n        mbar.graph_axs[0].plot(self.losses, '.', alpha=0.3)\n        mbar.graph_axs[0].set_yscale('log')\n        mbar.graph_axs[0].set_ylim(tensor(self.losses).min(), self.losses[0])\n        \n        # Show preview images:\n        rgb = learn.model.to_rgb(learn.preds.detach()).clip(0, 1)\n        show_image(torchvision.utils.make_grid(rgb), ax=mbar.graph_axs[1])\n        \n        # Update graph\n        mbar.graph_out.update(mbar.graph_fig)\n\nAlong with Style Loss which we defined above, we also use overflow loss which penalises predictions that overflow beyond a range (-1 to 1 in our case)\n\nclass NCACB(TrainCB):\n    order = DeviceCB.order+1\n    def __init__(self, ca, style_img_tensor, style_loss_scale=0.1, size=256, \n                 step_n_min=32, step_n_max=96, batch_size=4):\n        fc.store_attr()\n        with torch.no_grad(): self.pool = make_grids(256, sz=size) # Set up a 'pool' of grids\n    \n    def predict(self, learn): \n        \n        # Pick some random samples from the pool\n        #we select random 4 numbers to be the current batch and use those grid from the pool\n        batch_idx = torch.randint(0, len(self.pool), (self.batch_size,))\n        x = self.pool[batch_idx]\n        \n        # occasionally zero out some samples\n        #if a random number is less than 1 from selecting 1 to 7, it zeros out \n        if torch.randint(8, (1,)) &lt; 1: \n            x[:1] =  make_grids(1, sz=self.size)\n        \n        # Apply the model a number of times\n        for _ in range(torch.randint(self.step_n_min, self.step_n_max, (1,))):\n            x = learn.model(x)\n        \n        # Update pool\n        with torch.no_grad(): self.pool[batch_idx] = x\n        \n        # and store preds\n        learn.preds = x\n        \n    def get_loss(self, learn): \n        style_loss = learn.loss_func(learn.model.to_rgb(self.learn.preds))\n        overflow_loss = (learn.preds-learn.preds.clamp(-1.0, 1.0)).abs().sum()\n        learn.loss = overflow_loss + style_loss*self.style_loss_scale\n        \n    def backward(self, learn):\n        learn.loss.backward()\n        # Gradient normalization:\n        for p in learn.model.parameters():\n            p.grad /= (p.grad.norm()+1e-8) \n        \n    def before_fit(self, learn): self.learn=learn \n\n\nmodel = SimpleCA().to(def_device)\ncbs = [NCACB(model, style_im), NCAProgressCB(), MetricsCB()]\nstyle_loss = StyleLossToTarget(style_im)\n\nlearn = Learner(model, get_dummy_dls(1200), style_loss, lr=1e-3, cbs=cbs, opt_func=torch.optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n164.076\n0\ntrain\n\n\n26.250\n0\neval"
  },
  {
    "objectID": "posts/nca/index.html#results",
    "href": "posts/nca/index.html#results",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "Below is the final batch of images in training\n\n# Check out the final batch:\nrgb = model.to_rgb(learn.preds.detach())\nrgb = torchvision.utils.make_grid(rgb)\nshow_image(rgb.clip(0, 1));\n\n\n\n\n\n\n\n\nLet’s put our trained NCA model to use. We start of with a random noise grid as usual, and apply the model to this grid for n number of times(900 below). With a few hundred steps we see the patterns forming from our style image. If we look closely we can see how eloquently these patterns are forming, with even spacing and ending properly around edges. All this with a model with just a few hundred paramerers. Having the ability to capture regenerative, complex patterns with just few hundred parameters is the real beauty of these NCA self organising models.\n\n# Apply a numbe of times to a random initial starting grid:\nimages = []\nx = torch.randn(1, num_channels, 128, 128).to(def_device) * 0.1\nwith torch.no_grad():\n    for i in range(1200):\n        x = model(x)\n        if i%100==0: images.append(model.to_rgb(x)[0].clip(0, 1))\nshow_images(images)\n\n\n\n\n\n\n\n\nSo just with 168 weight parameters, we have a model which can create patterns of the style image from random noise\n\nsum(p.numel() for p in model.parameters()) # !!\n\n168"
  },
  {
    "objectID": "posts/nca/index.html#video",
    "href": "posts/nca/index.html#video",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "Let’s put our images we get from our model at different timesteps, into a video to see the progress of how the patterns develop over time as the grid goes from noise to changing some pixels to finally developing mature patterns present in the Style Image.\n\ndef progress_video():\n  # Turn the images in steps/ into a video with ffmpeg\n  !ffmpeg -y -v 0 -framerate 24 -i steps/%05d.jpeg video.mp4\n\n  # Display it inline\n  mp4 = open('video.mp4','rb').read()\n  data_url = \"data:video/mp4;base64,\" + base64.b64encode(mp4).decode()\n  return HTML(\"\"\"\n  &lt;video width=256 controls&gt;\n        &lt;source src=\"%s\" type=\"video/mp4\"&gt;\n  &lt;/video&gt;\n  \"\"\" % data_url)\n\n\n!mkdir -p steps\n!rm steps/*\nx = torch.randn(1, num_channels, 128, 128).to(def_device) * 0.1\nfor i in range(900):\n  with torch.no_grad():\n    x = model(x)\n    img = model.to_rgb(x).detach().cpu().clip(0, 1).squeeze().permute(1, 2, 0)\n    img = Image.fromarray(np.array(img*255).astype(np.uint8))\n    img.save(f'steps/{i:05}.jpeg')\nprogress_video()"
  },
  {
    "objectID": "posts/nca/index.html#conclusion",
    "href": "posts/nca/index.html#conclusion",
    "title": "Neural Cellular Automata",
    "section": "",
    "text": "In the post today, we tried out the vanilla NCA model. We saw the power of self organising systems.We made pretty pictures using differentiable self-organizing systems. There is a lot of scope to make this model detect and regenerate complex persisting patterns. We only saw a small glimpse of self organising systems. We can add more kernels, make the model deeper, try out different loss function among other improvements to see how complex patterns these models can capture, and regenerate. We can erase out part of the pattern from the image and still the model will be able to regenerate the pattern with just the knowledge of the neighbouring cells. We will try this and much more in the upcoming posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sehaj-notepad",
    "section": "",
    "text": "Latent Diffusion and Perceptual Latent Loss to Generate Church images\n\n\n\n\n\n\nAI\n\n\nLatent Diffusion\n\n\npython\n\n\njupyter\n\n\ncode\n\n\n\nLatent Diffusion to generate Church images.\n\n\n\n\n\nMar 20, 2024\n\n\nSehajdeep Singh\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Cellular Automata\n\n\n\n\n\n\nAI\n\n\npython\n\n\njupyter\n\n\ncode\n\n\n\nSelf-organizing systems of simple cells can produce complex patterns.\n\n\n\n\n\nFeb 5, 2024\n\n\nSehajdeep Singh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Sehaj, and this blog is my personal exploration into the depths of artificial intelligence and machine learning. Here, I share my insights, reflections, and opinions on the latest technologies, models, and papers that capture my curiosity."
  },
  {
    "objectID": "posts/latent_diffusion/index.html",
    "href": "posts/latent_diffusion/index.html",
    "title": "Latent Diffusion and Perceptual Latent Loss to Generate Church images",
    "section": "",
    "text": "In today’s post, we will build Latent Diffusion models to generate Church images at reduced training times using latents and using a pre-trained ImageNet latent classifier as a component to add perceptual loss. We will be using LSUN Church dataset, trained for 30 epochs on our U-Net model. We will first train a U-Net model with just MSE loss to sample our Church images, and then intorduce the a perceptual loss function in the mix to see how it affects the generated samples.The intuition behind using the Imagenet latent classifier to add perceptual loss is that the model has learnt about images in the latent space and its parameters have captured information about features of images, all in the compressed latent space.By focusing on high-level features, perceptual loss function can produce results that align better with human visual perception, leading to higher-quality image generation. Let’s dive into the notebook and understand the entire process piece by piece.\nThe notebook is structured as follows - We compress(encode) the LSUN Church dataset using sd-vae-ft-ema VAE.These latents map 3 channel 256 x 256 pixel images down by a spatial resolution factor of 8 to 4 channels 32 x 32 latents. - Then we add noise to these latent images using Linear Noise scheduler and train a U-Net model to predict noise in an image which is in the latent space using these encoded latents. First the model is trained just with MSE loss, then perceptual loss is added to create a combined loss function. The perceptual loss is added courtesy of a external network traied on the entire Imagenet Dataset in the latent space. - Using DDIM sampler, we generate Church images.\n\n\n\n!pip install -Uqq git+https://github.com/fastai/course22p2\n\n\n%%capture\n!pip install git+https://github.com/huggingface/transformers\n\n\n%%capture\n!pip install accelerate\n\n\nimport timm, torch, random, datasets, math, fastcore.all as fc, numpy as np, matplotlib as mpl, matplotlib.pyplot as plt\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader,default_collate\nfrom pathlib import Path\nfrom torch.nn import init\nfrom fastcore.foundation import L\nfrom torch import nn,tensor\nfrom datasets import load_dataset\nfrom operator import itemgetter\nfrom torcheval.metrics import MulticlassAccuracy\nfrom functools import partial\nfrom torch.optim import lr_scheduler\nfrom torch import optim\nfrom torchvision.io import read_image,ImageReadMode\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.init import *\nfrom miniai.sgd import *\n# from miniai.resnet import *\nfrom miniai.augment import *\nfrom miniai.accel import *\nfrom miniai.training import *\n\n\n# from miniai.imports import *\nfrom miniai.diffusion import *\n\nfrom glob import glob\nfrom fastprogress import progress_bar\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\n\n\nimport timm\n\n\ntorch.set_printoptions(precision=4, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray_r'\nmpl.rcParams['figure.dpi'] = 70\n\nset_seed(42)\nif fc.defaults.cpus&gt;8: fc.defaults.cpus=8\n\n\n\n\nWe will start off by compressing the LSUN Church dataset images into VAE encoded latents.\n\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath = path_data/'church'\n\n\nbs = 64\n\n\ndef to_img(f): return read_image(f, mode=ImageReadMode.RGB)/255\n\nWe read each file and convert it into a 256X256 image. Remember, VAE encodes 256X256 images into 32X32 in the latent space(and 64X64 for 512X512 images). We do need the images to be of same size to pass it to VAE decoder in batches.\n\nclass ImagesDS:\n    def __init__(self, spec):\n        self.path = Path(path)\n        self.files = glob(str(spec), recursive=True)\n    def __len__(self): return len(self.files)\n    def __getitem__(self, i): return to_img(self.files[i])[:, :256,:256]\n\n\nds = ImagesDS(path/f'**/*.jpeg')\n\n\nlen(ds)\n\n126227\n\n\nLet’s have a look into our dataset\n\ndl = DataLoader(ds, batch_size=bs, num_workers=fc.defaults.cpus)\nxb = next(iter(dl))\nshow_images(xb[:16], imsize=3)\n\n\n\n\n\n\n\n\n\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\").cuda().requires_grad_(False)\n\n\n\n\n\n\n\n\nxe = vae.encode(xb.cuda())\n\n\nxs = xe.latent_dist.mean[:16]\nxs.shape\n\ntorch.Size([16, 4, 32, 32])\n\n\nThe below cell shows that the encoded images are 48 times smaller than the original pixel images requiring 48 times less memory and less compute.\n\n(16*3*256*256)/(16*4*32*32)\n\n48.0\n\n\nLet’s visualise the images in the latent space\n\nshow_images(((xs[:16,:3])/4).sigmoid(), imsize=2)\n\n\n\n\n\n\n\n\nThe smaller encoded images are no good, if they can’t be transformed into their original form without losing its characteristics and visuals. We decode them using VAE and they look as good as the originals.\n\nxd = to_cpu(vae.decode(xs))\nshow_images(xd['sample'].clamp(0,1), imsize=3)\n\n\n\n\n\n\n\n\n\n\nWe are going to store our latent data into a memory mapped numpy file. Whatever memory numpy uses in RAM, is copied to the disk and all operations are written onto disk as well.Even though it is on disk, it still uses caching to get the data on RAM which you are using at a particular instance without comprimising speed.\n\nmmpath = Path('data/church/data.npmm')\n\nThe first dimension is the number of images in our dataset\n\nmmshape = (126227,4,32,32)\n\nWe go through the dataset, encode it as a (4,32,32) latent image, and save it on disk.\n\nif not mmpath.exists():\n    a = np.memmap(mmpath, np.float32, mode='w+', shape=mmshape)\n    i = 0\n    for b in progress_bar(dl):\n        n = len(b)\n        a[i:i+n] = to_cpu(vae.encode(b.cuda()).latent_dist.mean).numpy()\n        i += n\n    a.flush()\n    del(a)\n\n\n\n\n\n\n    \n      \n      100.00% [1973/1973 49:27&lt;00:00]\n    \n    \n\n\n\nlats = np.memmap(mmpath, dtype=np.float32, mode='r', shape=mmshape)\n\n\nb = torch.tensor(lats[:16])\n\n\nxd = to_cpu(vae.decode(b.cuda()))\nshow_images(xd['sample'].clamp(0,1), imsize=2)"
  },
  {
    "objectID": "posts/latent_diffusion/index.html#imports",
    "href": "posts/latent_diffusion/index.html#imports",
    "title": "Latent Diffusion and Perceptual Latent Loss to Generate Church images",
    "section": "",
    "text": "!pip install -Uqq git+https://github.com/fastai/course22p2\n\n\n%%capture\n!pip install git+https://github.com/huggingface/transformers\n\n\n%%capture\n!pip install accelerate\n\n\nimport timm, torch, random, datasets, math, fastcore.all as fc, numpy as np, matplotlib as mpl, matplotlib.pyplot as plt\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader,default_collate\nfrom pathlib import Path\nfrom torch.nn import init\nfrom fastcore.foundation import L\nfrom torch import nn,tensor\nfrom datasets import load_dataset\nfrom operator import itemgetter\nfrom torcheval.metrics import MulticlassAccuracy\nfrom functools import partial\nfrom torch.optim import lr_scheduler\nfrom torch import optim\nfrom torchvision.io import read_image,ImageReadMode\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.init import *\nfrom miniai.sgd import *\n# from miniai.resnet import *\nfrom miniai.augment import *\nfrom miniai.accel import *\nfrom miniai.training import *\n\n\n# from miniai.imports import *\nfrom miniai.diffusion import *\n\nfrom glob import glob\nfrom fastprogress import progress_bar\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\n\n\nimport timm\n\n\ntorch.set_printoptions(precision=4, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray_r'\nmpl.rcParams['figure.dpi'] = 70\n\nset_seed(42)\nif fc.defaults.cpus&gt;8: fc.defaults.cpus=8"
  },
  {
    "objectID": "posts/latent_diffusion/index.html#data",
    "href": "posts/latent_diffusion/index.html#data",
    "title": "Latent Diffusion and Perceptual Latent Loss to Generate Church images",
    "section": "",
    "text": "We will start off by compressing the LSUN Church dataset images into VAE encoded latents.\n\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath = path_data/'church'\n\n\nbs = 64\n\n\ndef to_img(f): return read_image(f, mode=ImageReadMode.RGB)/255\n\nWe read each file and convert it into a 256X256 image. Remember, VAE encodes 256X256 images into 32X32 in the latent space(and 64X64 for 512X512 images). We do need the images to be of same size to pass it to VAE decoder in batches.\n\nclass ImagesDS:\n    def __init__(self, spec):\n        self.path = Path(path)\n        self.files = glob(str(spec), recursive=True)\n    def __len__(self): return len(self.files)\n    def __getitem__(self, i): return to_img(self.files[i])[:, :256,:256]\n\n\nds = ImagesDS(path/f'**/*.jpeg')\n\n\nlen(ds)\n\n126227\n\n\nLet’s have a look into our dataset\n\ndl = DataLoader(ds, batch_size=bs, num_workers=fc.defaults.cpus)\nxb = next(iter(dl))\nshow_images(xb[:16], imsize=3)\n\n\n\n\n\n\n\n\n\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\").cuda().requires_grad_(False)\n\n\n\n\n\n\n\n\nxe = vae.encode(xb.cuda())\n\n\nxs = xe.latent_dist.mean[:16]\nxs.shape\n\ntorch.Size([16, 4, 32, 32])\n\n\nThe below cell shows that the encoded images are 48 times smaller than the original pixel images requiring 48 times less memory and less compute.\n\n(16*3*256*256)/(16*4*32*32)\n\n48.0\n\n\nLet’s visualise the images in the latent space\n\nshow_images(((xs[:16,:3])/4).sigmoid(), imsize=2)\n\n\n\n\n\n\n\n\nThe smaller encoded images are no good, if they can’t be transformed into their original form without losing its characteristics and visuals. We decode them using VAE and they look as good as the originals.\n\nxd = to_cpu(vae.decode(xs))\nshow_images(xd['sample'].clamp(0,1), imsize=3)\n\n\n\n\n\n\n\n\n\n\nWe are going to store our latent data into a memory mapped numpy file. Whatever memory numpy uses in RAM, is copied to the disk and all operations are written onto disk as well.Even though it is on disk, it still uses caching to get the data on RAM which you are using at a particular instance without comprimising speed.\n\nmmpath = Path('data/church/data.npmm')\n\nThe first dimension is the number of images in our dataset\n\nmmshape = (126227,4,32,32)\n\nWe go through the dataset, encode it as a (4,32,32) latent image, and save it on disk.\n\nif not mmpath.exists():\n    a = np.memmap(mmpath, np.float32, mode='w+', shape=mmshape)\n    i = 0\n    for b in progress_bar(dl):\n        n = len(b)\n        a[i:i+n] = to_cpu(vae.encode(b.cuda()).latent_dist.mean).numpy()\n        i += n\n    a.flush()\n    del(a)\n\n\n\n\n\n\n    \n      \n      100.00% [1973/1973 49:27&lt;00:00]\n    \n    \n\n\n\nlats = np.memmap(mmpath, dtype=np.float32, mode='r', shape=mmshape)\n\n\nb = torch.tensor(lats[:16])\n\n\nxd = to_cpu(vae.decode(b.cuda()))\nshow_images(xd['sample'].clamp(0,1), imsize=2)"
  },
  {
    "objectID": "posts/latent_diffusion/index.html#analzying-the-loss",
    "href": "posts/latent_diffusion/index.html#analzying-the-loss",
    "title": "Latent Diffusion and Perceptual Latent Loss to Generate Church images",
    "section": "Analzying the loss",
    "text": "Analzying the loss\nThe model is trained for a total of 30 epochs. One thing that catches the eye is the loss value being high. If we do Simple diffusion in pixel space, loss is much lower(into 0.03 after 15 epochs).The reason for the loss being high is the complexity of pixel generation in the latent space. - Pixel space values directly represent the visual appearance of images and are highly correlated with features such as shapes and textures, where as latent space values provide a more interpretable and lower-dimensional representation of the underlying features and attributes of the data. - So generating image pixels are much easier as the nearby pixels will have similar values, the background might be same. When you try to generate latent pixels, the task is much more precise about what you want to generate and the error goes up as it is a much more difficult task for the model"
  },
  {
    "objectID": "posts/latent_diffusion/index.html#ddim-sampler",
    "href": "posts/latent_diffusion/index.html#ddim-sampler",
    "title": "Latent Diffusion and Perceptual Latent Loss to Generate Church images",
    "section": "DDIM Sampler",
    "text": "DDIM Sampler\n\n\nA sampling step in a diffusion model consists of: - predicting the direction in input space in which we should move to remove noise, or equivalently, to make the input more likely under the data distribution; - taking a small step in that direction.\nThe above image shows the main equations that make up the DDIM paper. The σ(sigma) in the above equations is what makes this algorithm deterministic(and the difference from the probalistic predecessor DDPM). As we run this DDIM algorithm once, we get a direction towards the target(input) data distribution and take a step towards that direction.This direction is just an estimate of where our actual data distriution is and is not necessarily the absolute right direction. If we proceed to take a step in this direction and add some noise (as we do in the DDPM sampling algorithm, for example), we end up with \\(x_{t-1}\\),which corresponds to a slightly less noisy input image. The predicted direction now points to a smaller, “more specific” region of high likelihood, because some uncertainty was resolved by the previous sampling step. We add the noise back to this predicted direction so that the model can explore different regions of the data distribution.\nIn DDIM, the σ(sigma) controls how muchh noise we add which makes the process deterministic. As we can see from the second equation in the above image, the “random noise” σ\\(_{t}\\)ε\\(_{t}\\) is added back. If σ is zero the algorithm becomes completely deterministic and if σ is 1 it is just DDPM. Despite the stochastic nature of adding noise during sampling, the overall sampling process in DDIM can still be considered deterministic in the sense that given the same input (noisy observation) and the same diffusion parameters, the output (sampled image) will be consistent and reproducible.\n\ncheckpoint = torch.load(\"models/church_mseonly\")\n\n\nmodel = EmbUNetModel(in_channels=4, out_channels=4, nfs=(128, 256, 512, 768), num_layers=2,\n                     attn_start=1, attn_chans=16)\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\n&lt;All keys matched successfully&gt;\n\n\n\nmodel = model.cuda() \n\n\nsz = (16,4,32,32)\n\n\nmodel = model.cuda()\n# set_seed(42)\npreds = sample(ddim_step, model, sz, steps=100, eta=1., clamp=False)\n\n\n\n\n\n\n    \n      \n      100.00% [100/100 00:02&lt;00:00]\n    \n    \n\n\n\ns = preds[-1]*5\n\n\nwith torch.no_grad(): pd = to_cpu(vae.decode(s.cuda()))\n\nThe samples generated were able to capture the nuances of the scene including the main Church, the background and foreground settings. In some of the generated samples it was able to capture details of humans,or human crowds in the foreground. This is quite remarkable for a model trained just for 2-3 hours on a A1000 gpu card from scratch.\n\nshow_images(pd['sample'][:16].clamp(0,1), imsize=5)\n\n\n\n\n\n\n\n\n## Humans in the foreground\nLet’s look at this particular sample, where we can see the entire scene of a Church in front of a lake, where boat like structures, flowers and humans on the edge of the boat on the land seem to appear. We can see what looks like a crowd of few people, though the feature details seem to be missing. Nevertheless, the amount of object detail it has captured in the sampling is a huge positive.\n\nshow_images(pd['sample'][9:10].clamp(0,1), imsize=5)\n\n\n\n\n\n\n\n\n\nshow_images(pd['sample'][:16].clamp(0,1), imsize=5)\n\n\n\n\n\n\n\n\n\nshow_images(pd['sample'][:45].clamp(0,1), imsize=5)"
  },
  {
    "objectID": "posts/latent_diffusion/index.html#conclusion",
    "href": "posts/latent_diffusion/index.html#conclusion",
    "title": "Latent Diffusion and Perceptual Latent Loss to Generate Church images",
    "section": "Conclusion",
    "text": "Conclusion\nToday, we generated Church images by training 2 diffusion models, one with MSE loss and the other with added perceptual loss. The pieces we scraped together to generate images and train these models were our very own. Considering the dataset we used was very small(for a generative model trained from scratch), and each of the model trained for just 3-4 hours on a 16Gb A1000 card, we got promising results.\nSome work has been come forward which uses the stable diffusion model itslef as the model to calculate perceptual loss on. The technique utilizes the diffusion model itself to generate meaningful perceptual loss."
  }
]